{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRDM_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOfdN7MwFYRrAu1ljn6uARP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apof/Information-Retrieval---Data-Mining-/blob/main/IRDM_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qvy-qmt-Pm",
        "outputId": "06cb8b7c-9ec8-4d8a-bc60-60e3eaa50fab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzlwMzcmb7bw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDu6NhklusF"
      },
      "source": [
        "**Load The Data and Process the with Pandas DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQwDZDKHuH1l"
      },
      "source": [
        "train_data_file = \"drive/MyDrive/Datasets/IRDM/train_data.tsv\"\n",
        "test_data_file = \"drive/MyDrive/Datasets/IRDM/validation_data.tsv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btO5-N2ybl2O",
        "outputId": "8b450150-f0c4-4f66-9fdd-4b9d06fc4e05"
      },
      "source": [
        "col_names=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data_init=pd.read_csv(train_data_file, sep='\\t', header=None, names=col_names)\n",
        "train_data_df=pd.DataFrame(train_data_init)\n",
        "train_data_df = train_data_df.iloc[1:]\n",
        "\n",
        "test_data_init=pd.read_csv(test_data_file, sep='\\t', header=None, names=col_names)\n",
        "test_data_df=pd.DataFrame(test_data_init)\n",
        "test_data_df = test_data_df.iloc[1:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2182zoCaeOy0"
      },
      "source": [
        "train_data_df[\"relevancy\"] = train_data_df.relevancy.astype(float)\n",
        "test_data_df[\"relevancy\"] = test_data_df.relevancy.astype(float)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWdiR4ehWr5"
      },
      "source": [
        "train_labels = train_data_df['relevancy'].values\n",
        "test_labels = test_data_df['relevancy'].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DclKom1jTXP"
      },
      "source": [
        "train_passages = train_data_df['passage'].values\n",
        "train_queries = train_data_df['query'].values\n",
        "train_pids = train_data_df['pid'].values\n",
        "train_qids = train_data_df['qid'].values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAxbJF1mtw8"
      },
      "source": [
        "test_passages = test_data_df['passage'].values\n",
        "test_queries = test_data_df['query'].values\n",
        "test_pids = test_data_df['pid'].values\n",
        "test_qids = test_data_df['qid'].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpukHGbvzKb2"
      },
      "source": [
        "**Create Dictionaries for the Training and Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9soNcNQGzIFn"
      },
      "source": [
        "train_passage_dict = {}\n",
        "train_query_dict = {}\n",
        "for i, _ in enumerate(train_queries):\n",
        "  train_passage_dict[train_pids[i]] = train_passages[i]\n",
        "  train_query_dict[train_qids[i]] = train_queries[i]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJz1sCG433R8"
      },
      "source": [
        "test_passage_dict = {}\n",
        "test_query_dict = {}\n",
        "for i, _ in enumerate(test_queries):\n",
        "  test_passage_dict[test_pids[i]] = test_passages[i]\n",
        "  test_query_dict[test_qids[i]] = test_queries[i]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx4snFI1thV0"
      },
      "source": [
        "**Correct Imbalanced Data with Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nozDa9ttqD"
      },
      "source": [
        "def collect_relevant_pids(pids,qids,relevancy):\n",
        "\n",
        "  relevancy_dict = {}\n",
        "  ## collect for each query all the relevant passages (relevant passages is the minority class)\n",
        "  ## and then sample randomply K negative examples\n",
        "\n",
        "  for i, qid in enumerate(qids):\n",
        "    if(relevancy_dict.get(qid) == None):\n",
        "      if(relevancy[i] == 0):\n",
        "        relevancy_dict[qid] = [[],[(pids[i],i)]]\n",
        "      else:\n",
        "        relevancy_dict[qid] = [[(pids[i],i)],[]]\n",
        "    else:\n",
        "      l = relevancy_dict.get(qid)\n",
        "      if(relevancy[i] == 0):\n",
        "        l[1].append((pids[i],i))\n",
        "      else:\n",
        "        l[0].append((pids[i],i))\n",
        "      relevancy_dict[qid] = l\n",
        "\n",
        "  return relevancy_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Sb4rSpvP9n"
      },
      "source": [
        "train_relevant_passages_dict = collect_relevant_pids(train_pids,train_qids,train_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZa3r6YL4hpt"
      },
      "source": [
        "test_relevant_passages_dict = collect_relevant_pids(test_pids,test_qids,test_labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQXi92iZwCKf"
      },
      "source": [
        "def negative_sampling(relevant_passages_dict,passage_dict,query_dict,K):\n",
        "\n",
        "  sampled_passages = []\n",
        "  sampled_queries = []\n",
        "  sampled_labels = []\n",
        "  sampled_qids = []\n",
        "  sampled_pids = []\n",
        "\n",
        "\n",
        "  for key in relevant_passages_dict:\n",
        "    l = relevant_passages_dict.get(key)\n",
        "\n",
        "    number_of_positive_samples = len(l[0])\n",
        "    number_of_negative_passages = len(l[1])\n",
        "\n",
        "    #print(\"query \" + str(key) + \" has \" + str(number_of_positive_samples) + \" positive and \" + str(number_of_negative_passages) + \" negative sample\" )\n",
        "\n",
        "    ## if there are not relevant passages continue\n",
        "    if(number_of_positive_samples != 0):\n",
        "      ## collect all relevant passages\n",
        "      for (pid,_) in l[0]:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(pid))\n",
        "        sampled_labels.append(1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(pid)\n",
        "\n",
        "      negative_examples_to_draw = None\n",
        "      ## if the positive examples for a query are more than the negative one --> select all the negative and add them to the dataset\n",
        "      if(number_of_positive_samples >= number_of_negative_passages):\n",
        "        negative_examples_to_draw = number_of_negative_passages\n",
        "      else:\n",
        "        ## if there are more negative examples than K\n",
        "        if(number_of_negative_passages >= K):\n",
        "          negative_examples_to_draw = K\n",
        "        else:\n",
        "          ## elese collect all the negaative examples\n",
        "          negative_examples_to_draw = number_of_positive_samples\n",
        "\n",
        "      ## select a number of negative samples equal to the number of the positive ones\n",
        "      indexes = random.sample(range(0,number_of_negative_passages), negative_examples_to_draw)\n",
        "\n",
        "      for i in indexes:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(l[1][i][0]))\n",
        "        sampled_labels.append(-1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(l[1][i][0])\n",
        "\n",
        "  return sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sLnaKNvpGDL"
      },
      "source": [
        "## how many negative samples to collect for each positive one\n",
        "K = 5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEc_zfRh3LLJ"
      },
      "source": [
        "sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids = negative_sampling(train_relevant_passages_dict,train_passage_dict,train_query_dict,K)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp5DvgR8_GZ",
        "outputId": "9c42e590-1762-4783-e8c9-75c537e2d844"
      },
      "source": [
        "sampled_labels = np.array(sampled_labels)\n",
        "print(np.where(sampled_labels == 1)[0].shape)\n",
        "print(np.where(sampled_labels == -1)[0].shape)\n",
        "\n",
        "print(sampled_labels.shape[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4797,)\n",
            "(22916,)\n",
            "27713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDdJnfsnlaQ"
      },
      "source": [
        "**Preprocess Data and data Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmEm2R_oL-w",
        "outputId": "e7a1de03-907c-4467-e587-39c089b6d1ed"
      },
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import  WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CC1oGrD75zr"
      },
      "source": [
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4RW-o2n_9n",
        "outputId": "e1bd3453-469b-4335-bb89-d1637c834d0e"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-02 08:45:51--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.162.0\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.162.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  78.0MB/s    in 23s     \n",
            "\n",
            "2021-04-02 08:46:14 (69.6 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiZeCynoF9n"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8VdP2q4ogG"
      },
      "source": [
        "def process_data(data,rm_stopwords = True, lemm = True):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\t## remove punctuation\n",
        "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
        "\tprocessed_sentences = []\n",
        "\tfor sentence in data:\n",
        "\t\t## tokenise each sentence\n",
        "\t\ttokenised_sentence = tokenizer.tokenize(sentence)\n",
        "\t\t##convert to lower case\n",
        "\t\tsentence = [w.lower() for w in tokenised_sentence]\n",
        "\t\t##exclude non alphabetic words\n",
        "\t\tonly_alpha_sentence = [word for word in sentence if word.isalpha()]\n",
        "\t\t## remove stop words\n",
        "\t\tif(rm_stopwords == True):\n",
        "\t\t\tfiltered_sentence = [w for w in only_alpha_sentence if not w in stop_words]\n",
        "\t\telse:\n",
        "\t\t\tfiltered_sentence = only_alpha_sentence\n",
        "\t\tlemmatized_sentence = []\n",
        "\t\tif(lemm == True):\n",
        "\t\t\t## stemming\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\t#stemmer = PorterStemmer()\n",
        "\t\t\tfor word in filtered_sentence:\n",
        "\t\t\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "\t\t\t\t#lemmatized_sentence.append(stemmer.stem(word))\n",
        "\t\telse:\n",
        "\t\t\tlemmatized_sentence = filtered_sentence\n",
        "\n",
        "\t\tprocessed_sentences.append(lemmatized_sentence)\n",
        "\treturn processed_sentences"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvZjgioo0Nc"
      },
      "source": [
        "train_passages = process_data(sampled_passages)\n",
        "train_queries = process_data(sampled_queries)\n",
        "test_passages = process_data(test_passages)\n",
        "test_queries = process_data(test_queries)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRqgIdl6nDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8107b0a1-7107-4c97-e284-45c0260c0640"
      },
      "source": [
        "print(len(train_passages))\n",
        "print(len(train_queries))\n",
        "\n",
        "print(len(test_passages))\n",
        "print(len(test_queries))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27713\n",
            "27713\n",
            "1103039\n",
            "1103039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlfzlqeoSpbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153298aa-3ce8-4093-f510-db516bb31673"
      },
      "source": [
        "print(train_passages[0])\n",
        "print(train_queries[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['disco', 'sensation', 'andy', 'gibb', 'dy', 'age', 'knee', 'buckling', 'good', 'look', 'brother', 'songwriting', 'talent', 'backing', 'year', 'old', 'andy', 'gibb', 'staged', 'unprecedented', 'display', 'youthful', 'pop', 'mastery', 'month', 'following', 'american', 'debut', 'spring']\n",
            "['andy', 'gibb', 'die']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcfd6e3pXAXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb0b4d2-114c-4452-fe8d-29979147aac2"
      },
      "source": [
        "max_l = 0\n",
        "for sentence in train_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in train_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "print(max_l)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPx8SdpoXk-o"
      },
      "source": [
        "def find_unique_words(datasets):\n",
        "\n",
        "  # words to indexes\n",
        "  vocab_words = {}\n",
        "  # indexes to embeddings\n",
        "  vocab_embeddings = {}\n",
        "\n",
        "  unique_words = 0\n",
        "\n",
        "  # for every data set training validation testing\n",
        "  for dataset in datasets:\n",
        "    # for every sentence\n",
        "    for sentence in dataset:\n",
        "      # for each word of the sentence\n",
        "      for word in sentence:\n",
        "        # if this word is not saved on the dictionary\n",
        "        if(vocab_words.get(word)==None):\n",
        "          # if this word exists is the word Word2Vec model\n",
        "          if word in word2vec:\n",
        "            unique_words += 1\n",
        "            vocab_words[word] = unique_words\n",
        "            vocab_embeddings[unique_words] = word2vec.wv[word]\n",
        "\n",
        "  return vocab_words,vocab_embeddings"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwTVq4HaXTBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aeebc1c-c24b-428e-dbaf-fae2c98b4282"
      },
      "source": [
        "## vocab dict --> key each unique word and value a unique id\n",
        "## embeddings dict --> key the unique id of a word and value the respective embedding\n",
        "vocab_dict,vocab_embeddings_dict = find_unique_words([train_passages,train_queries,test_passages,test_queries])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5el6sTEm1s8o"
      },
      "source": [
        "def get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,sentences):\n",
        "  representations = []\n",
        "  ## for every sentence\n",
        "  for sentence in sentences:\n",
        "    sentence_representation = []\n",
        "    for word in sentence:\n",
        "      ## collect all the word embeddings of the respective words of a sentence\n",
        "      word_index = vocab_dict.get(word)\n",
        "      if(word_index!=None):\n",
        "        word_embedding = vocab_embeddings_dict.get(word_index)\n",
        "        sentence_representation.append(np.array(word_embedding))\n",
        "      else:\n",
        "        sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    if(len(sentence_representation) == 0):\n",
        "      sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    sentence_representation = np.array(sentence_representation)\n",
        "\n",
        "    representations.append(np.mean(sentence_representation,axis = 0))\n",
        "\n",
        "  return np.array(representations)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv20wAJO34tJ"
      },
      "source": [
        "train_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_passages)\n",
        "train_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_queries)\n",
        "\n",
        "test_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_passages)\n",
        "test_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_queries)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTeibggV6cdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3aaebe8-5ae7-4a46-d306-bdefb9a97dea"
      },
      "source": [
        "print(train_passage_represenations.shape)\n",
        "print(train_queries_represenations.shape)\n",
        "\n",
        "print(test_passage_represenations.shape)\n",
        "print(test_queries_represenations.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27713, 300)\n",
            "(27713, 300)\n",
            "(1103039, 300)\n",
            "(1103039, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LKo-hg-bIv"
      },
      "source": [
        "def combine_representations(q_rep,p_rep,flag = 'diff'):\n",
        "  representation = []\n",
        "  for i,p in enumerate(p_rep):\n",
        "    if(flag == 'diff'):\n",
        "      representation.append((p - q_rep[i])**2)\n",
        "    else:\n",
        "      representation.append(np.concatenate((p,q_rep[i]),axis = 0))\n",
        "  return np.array(representation)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb4lB_qq_jvX"
      },
      "source": [
        "train_inputs = combine_representations(train_queries_represenations,train_passage_represenations,flag = 'diff')\n",
        "\n",
        "test_inputs = combine_representations(test_queries_represenations,test_passage_represenations,flag = 'diff')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f76oWE8w2Khr"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI4CKPXZ6yyN"
      },
      "source": [
        "def predict(w,xTr):\n",
        "  return xTr@w"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7J9iGOa2O5E"
      },
      "source": [
        "def logistic(w,xTr,yTr):\n",
        "\n",
        "    n, d = xTr.shape\n",
        "    ## compute the logistic loss\n",
        "    loss = 0\n",
        "    for i in range(0,n):\n",
        "        loss += np.log(1 + np.exp(-yTr[i] * w.dot(xTr[i])))\n",
        "\n",
        "    ## compute the gradient\n",
        "    grad = np.zeros((d))\n",
        "    \n",
        "    for i in range(0,n):\n",
        "        term1 = (1/(1 + np.exp(-yTr[i] * w.dot(xTr[i]))))\n",
        "        term2 = np.exp(-yTr[i] * w.dot(xTr[i]))\n",
        "        term3 = (-yTr[i] * xTr[i])      \n",
        "        grad_term = term1 * term2 * term3       \n",
        "        grad  = np.add(grad,grad_term)\n",
        "                   \n",
        "    return loss, grad"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQ8QLi05KkB"
      },
      "source": [
        "def grad_descent(func,w,alpha,maxiter,tol=1e-02):\n",
        "    \n",
        "    losses = []\n",
        "    eps = 1e-06\n",
        "    G_matrix = np.zeros((w.shape[0],w.shape[0]))\n",
        "    \n",
        "    iter_num = 0\n",
        "    criterion = True\n",
        "    \n",
        "    while(iter_num < maxiter and criterion == True):\n",
        "        iter_num += 1\n",
        "        \n",
        "        loss, gradient = func(w)\n",
        "        print(\"Iteration: \" + str(iter_num) + \" with loss \" + str(loss))        \n",
        "        for i in range(w.shape[0]):\n",
        "            G_matrix[i][i] += gradient[i]**2\n",
        "        for i in range(w.shape[0]):\n",
        "            w[i] -= alpha*gradient[i]\n",
        "        losses.append(loss)\n",
        "        if(np.linalg.norm(gradient) < tol):\n",
        "            criterion = False\n",
        "        \n",
        "    return w, losses"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqmWMIAd5qJD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1afd743-461a-4ee4-8452-d66ee077898c"
      },
      "source": [
        "_, d = train_inputs.shape\n",
        "w, losses = grad_descent(lambda weight: logistic(weight, train_inputs, sampled_labels), np.random.rand(d), 0.0001, 500)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.semilogy(losses, c='r', linestyle='-')\n",
        "plt.xlabel(\"gradient updates\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Adagrad convergence\")\n",
        "print(\"Final train loss: %f\" % losses[-1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1 with loss 36321.16497682806\n",
            "Iteration: 2 with loss 35727.4591906862\n",
            "Iteration: 3 with loss 35142.93443895528\n",
            "Iteration: 4 with loss 34567.693575420184\n",
            "Iteration: 5 with loss 34001.834858540504\n",
            "Iteration: 6 with loss 33445.45166244369\n",
            "Iteration: 7 with loss 32898.63219643148\n",
            "Iteration: 8 with loss 32361.45923496921\n",
            "Iteration: 9 with loss 31834.009860030437\n",
            "Iteration: 10 with loss 31316.355217565822\n",
            "Iteration: 11 with loss 30808.560289636484\n",
            "Iteration: 12 with loss 30310.683683499305\n",
            "Iteration: 13 with loss 29822.777438593876\n",
            "Iteration: 14 with loss 29344.88685192967\n",
            "Iteration: 15 with loss 28877.050321898456\n",
            "Iteration: 16 with loss 28419.29920992614\n",
            "Iteration: 17 with loss 27971.65771875045\n",
            "Iteration: 18 with loss 27534.142785383574\n",
            "Iteration: 19 with loss 27106.76398608819\n",
            "Iteration: 20 with loss 26689.523449940585\n",
            "Iteration: 21 with loss 26282.415776850852\n",
            "Iteration: 22 with loss 25885.42795526391\n",
            "Iteration: 23 with loss 25498.539274293285\n",
            "Iteration: 24 with loss 25121.721224756227\n",
            "Iteration: 25 with loss 24754.93738358247\n",
            "Iteration: 26 with loss 24398.143276460476\n",
            "Iteration: 27 with loss 24051.28621438439\n",
            "Iteration: 28 with loss 23714.30510112094\n",
            "Iteration: 29 with loss 23387.130210529598\n",
            "Iteration: 30 with loss 23069.682935228535\n",
            "Iteration: 31 with loss 22761.875511325477\n",
            "Iteration: 32 with loss 22463.610727832944\n",
            "Iteration: 33 with loss 22174.781633860952\n",
            "Iteration: 34 with loss 21895.27126158994\n",
            "Iteration: 35 with loss 21624.952387895948\n",
            "Iteration: 36 with loss 21363.687361531458\n",
            "Iteration: 37 with loss 21111.328024633534\n",
            "Iteration: 38 with loss 20867.715754942103\n",
            "Iteration: 39 with loss 20632.68164600767\n",
            "Iteration: 40 with loss 20406.04682441319\n",
            "Iteration: 41 with loss 20187.6228749577\n",
            "Iteration: 42 with loss 19977.212310344315\n",
            "Iteration: 43 with loss 19774.6089917474\n",
            "Iteration: 44 with loss 19579.598399682225\n",
            "Iteration: 45 with loss 19391.957693957364\n",
            "Iteration: 46 with loss 19211.455601120833\n",
            "Iteration: 47 with loss 19037.852312255654\n",
            "Iteration: 48 with loss 18870.899704990185\n",
            "Iteration: 49 with loss 18710.342236664044\n",
            "Iteration: 50 with loss 18555.91873300608\n",
            "Iteration: 51 with loss 18407.36504449969\n",
            "Iteration: 52 with loss 18264.417272984047\n",
            "Iteration: 53 with loss 18126.81511235322\n",
            "Iteration: 54 with loss 17994.304858113148\n",
            "Iteration: 55 with loss 17866.641783983745\n",
            "Iteration: 56 with loss 17743.591771201547\n",
            "Iteration: 57 with loss 17624.93223089051\n",
            "Iteration: 58 with loss 17510.45244845634\n",
            "Iteration: 59 with loss 17399.953506117723\n",
            "Iteration: 60 with loss 17293.247927339293\n",
            "Iteration: 61 with loss 17190.159157125367\n",
            "Iteration: 62 with loss 17090.52095973887\n",
            "Iteration: 63 with loss 16994.1767877311\n",
            "Iteration: 64 with loss 16900.97915541283\n",
            "Iteration: 65 with loss 16810.789035602324\n",
            "Iteration: 66 with loss 16723.475289253107\n",
            "Iteration: 67 with loss 16638.914131922193\n",
            "Iteration: 68 with loss 16556.98863776673\n",
            "Iteration: 69 with loss 16477.58827996473\n",
            "Iteration: 70 with loss 16400.60850558118\n",
            "Iteration: 71 with loss 16325.950342520084\n",
            "Iteration: 72 with loss 16253.520036134621\n",
            "Iteration: 73 with loss 16183.228713125469\n",
            "Iteration: 74 with loss 16114.992070501028\n",
            "Iteration: 75 with loss 16048.73008754272\n",
            "Iteration: 76 with loss 15984.366758883729\n",
            "Iteration: 77 with loss 15921.82984697544\n",
            "Iteration: 78 with loss 15861.050652371558\n",
            "Iteration: 79 with loss 15801.963800394906\n",
            "Iteration: 80 with loss 15744.507042878187\n",
            "Iteration: 81 with loss 15688.621073789256\n",
            "Iteration: 82 with loss 15634.249357644441\n",
            "Iteration: 83 with loss 15581.337969720571\n",
            "Iteration: 84 with loss 15529.835447145659\n",
            "Iteration: 85 with loss 15479.69265003601\n",
            "Iteration: 86 with loss 15430.862631913846\n",
            "Iteration: 87 with loss 15383.300518700298\n",
            "Iteration: 88 with loss 15336.96339563989\n",
            "Iteration: 89 with loss 15291.810201565564\n",
            "Iteration: 90 with loss 15247.801629958167\n",
            "Iteration: 91 with loss 15204.90003630263\n",
            "Iteration: 92 with loss 15163.069351279955\n",
            "Iteration: 93 with loss 15122.274999373856\n",
            "Iteration: 94 with loss 15082.483822502836\n",
            "Iteration: 95 with loss 15043.664008318401\n",
            "Iteration: 96 with loss 15005.78502284298\n",
            "Iteration: 97 with loss 14968.817547141185\n",
            "Iteration: 98 with loss 14932.733417745494\n",
            "Iteration: 99 with loss 14897.505570578727\n",
            "Iteration: 100 with loss 14863.107988133283\n",
            "Iteration: 101 with loss 14829.515649689818\n",
            "Iteration: 102 with loss 14796.704484369398\n",
            "Iteration: 103 with loss 14764.651326832834\n",
            "Iteration: 104 with loss 14733.333875452741\n",
            "Iteration: 105 with loss 14702.730652797492\n",
            "Iteration: 106 with loss 14672.820968277929\n",
            "Iteration: 107 with loss 14643.584882819781\n",
            "Iteration: 108 with loss 14615.003175430242\n",
            "Iteration: 109 with loss 14587.057311545084\n",
            "Iteration: 110 with loss 14559.729413041181\n",
            "Iteration: 111 with loss 14533.002229815063\n",
            "Iteration: 112 with loss 14506.859112829801\n",
            "Iteration: 113 with loss 14481.283988543164\n",
            "Iteration: 114 with loss 14456.261334633486\n",
            "Iteration: 115 with loss 14431.776156946544\n",
            "Iteration: 116 with loss 14407.813967592534\n",
            "Iteration: 117 with loss 14384.360764124203\n",
            "Iteration: 118 with loss 14361.40300973606\n",
            "Iteration: 119 with loss 14338.927614424269\n",
            "Iteration: 120 with loss 14316.921917055011\n",
            "Iteration: 121 with loss 14295.373668287071\n",
            "Iteration: 122 with loss 14274.271014304939\n",
            "Iteration: 123 with loss 14253.602481314005\n",
            "Iteration: 124 with loss 14233.356960759254\n",
            "Iteration: 125 with loss 14213.523695225054\n",
            "Iteration: 126 with loss 14194.092264983394\n",
            "Iteration: 127 with loss 14175.052575151338\n",
            "Iteration: 128 with loss 14156.39484342856\n",
            "Iteration: 129 with loss 14138.109588384381\n",
            "Iteration: 130 with loss 14120.187618264177\n",
            "Iteration: 131 with loss 14102.62002028881\n",
            "Iteration: 132 with loss 14085.398150423469\n",
            "Iteration: 133 with loss 14068.513623589071\n",
            "Iteration: 134 with loss 14051.958304296808\n",
            "Iteration: 135 with loss 14035.724297682438\n",
            "Iteration: 136 with loss 14019.803940922458\n",
            "Iteration: 137 with loss 14004.189795010207\n",
            "Iteration: 138 with loss 13988.874636879234\n",
            "Iteration: 139 with loss 13973.85145185113\n",
            "Iteration: 140 with loss 13959.113426397793\n",
            "Iteration: 141 with loss 13944.65394119852\n",
            "Iteration: 142 with loss 13930.466564480337\n",
            "Iteration: 143 with loss 13916.54504562905\n",
            "Iteration: 144 with loss 13902.883309054885\n",
            "Iteration: 145 with loss 13889.475448303861\n",
            "Iteration: 146 with loss 13876.31572040377\n",
            "Iteration: 147 with loss 13863.398540430591\n",
            "Iteration: 148 with loss 13850.718476289543\n",
            "Iteration: 149 with loss 13838.27024369857\n",
            "Iteration: 150 with loss 13826.048701364023\n",
            "Iteration: 151 with loss 13814.04884634522\n",
            "Iteration: 152 with loss 13802.265809591217\n",
            "Iteration: 153 with loss 13790.694851651242\n",
            "Iteration: 154 with loss 13779.331358543111\n",
            "Iteration: 155 with loss 13768.170837779711\n",
            "Iteration: 156 with loss 13757.208914540448\n",
            "Iteration: 157 with loss 13746.441327986458\n",
            "Iteration: 158 with loss 13735.863927710472\n",
            "Iteration: 159 with loss 13725.47267031678\n",
            "Iteration: 160 with loss 13715.263616125423\n",
            "Iteration: 161 with loss 13705.232925995275\n",
            "Iteration: 162 with loss 13695.376858262149\n",
            "Iteration: 163 with loss 13685.691765785406\n",
            "Iteration: 164 with loss 13676.174093099806\n",
            "Iteration: 165 with loss 13666.820373668357\n",
            "Iteration: 166 with loss 13657.627227230825\n",
            "Iteration: 167 with loss 13648.59135724616\n",
            "Iteration: 168 with loss 13639.709548422217\n",
            "Iteration: 169 with loss 13630.978664332573\n",
            "Iteration: 170 with loss 13622.395645113209\n",
            "Iteration: 171 with loss 13613.957505240345\n",
            "Iteration: 172 with loss 13605.66133138137\n",
            "Iteration: 173 with loss 13597.504280320481\n",
            "Iteration: 174 with loss 13589.483576952875\n",
            "Iteration: 175 with loss 13581.59651234626\n",
            "Iteration: 176 with loss 13573.840441867407\n",
            "Iteration: 177 with loss 13566.212783370684\n",
            "Iteration: 178 with loss 13558.711015444618\n",
            "Iteration: 179 with loss 13551.332675719019\n",
            "Iteration: 180 with loss 13544.075359224713\n",
            "Iteration: 181 with loss 13536.93671680723\n",
            "Iteration: 182 with loss 13529.914453592486\n",
            "Iteration: 183 with loss 13523.0063274999\n",
            "Iteration: 184 with loss 13516.210147806294\n",
            "Iteration: 185 with loss 13509.523773751507\n",
            "Iteration: 186 with loss 13502.945113190535\n",
            "Iteration: 187 with loss 13496.472121288196\n",
            "Iteration: 188 with loss 13490.102799253822\n",
            "Iteration: 189 with loss 13483.835193115758\n",
            "Iteration: 190 with loss 13477.667392533756\n",
            "Iteration: 191 with loss 13471.597529648328\n",
            "Iteration: 192 with loss 13465.62377796582\n",
            "Iteration: 193 with loss 13459.744351276251\n",
            "Iteration: 194 with loss 13453.957502605854\n",
            "Iteration: 195 with loss 13448.261523199748\n",
            "Iteration: 196 with loss 13442.654741537444\n",
            "Iteration: 197 with loss 13437.135522374669\n",
            "Iteration: 198 with loss 13431.702265817652\n",
            "Iteration: 199 with loss 13426.353406421813\n",
            "Iteration: 200 with loss 13421.087412319419\n",
            "Iteration: 201 with loss 13415.902784371394\n",
            "Iteration: 202 with loss 13410.798055344345\n",
            "Iteration: 203 with loss 13405.771789112772\n",
            "Iteration: 204 with loss 13400.822579883903\n",
            "Iteration: 205 with loss 13395.94905144317\n",
            "Iteration: 206 with loss 13391.149856425222\n",
            "Iteration: 207 with loss 13386.423675602071\n",
            "Iteration: 208 with loss 13381.769217194238\n",
            "Iteration: 209 with loss 13377.185216199996\n",
            "Iteration: 210 with loss 13372.670433744965\n",
            "Iteration: 211 with loss 13368.223656448383\n",
            "Iteration: 212 with loss 13363.843695808959\n",
            "Iteration: 213 with loss 13359.529387606768\n",
            "Iteration: 214 with loss 13355.279591321989\n",
            "Iteration: 215 with loss 13351.093189570334\n",
            "Iteration: 216 with loss 13346.969087554056\n",
            "Iteration: 217 with loss 13342.90621252642\n",
            "Iteration: 218 with loss 13338.903513273772\n",
            "Iteration: 219 with loss 13334.95995960926\n",
            "Iteration: 220 with loss 13331.074541881637\n",
            "Iteration: 221 with loss 13327.246270496815\n",
            "Iteration: 222 with loss 13323.4741754528\n",
            "Iteration: 223 with loss 13319.757305886536\n",
            "Iteration: 224 with loss 13316.094729633092\n",
            "Iteration: 225 with loss 13312.48553279759\n",
            "Iteration: 226 with loss 13308.928819336383\n",
            "Iteration: 227 with loss 13305.423710651625\n",
            "Iteration: 228 with loss 13301.969345194664\n",
            "Iteration: 229 with loss 13298.564878081374\n",
            "Iteration: 230 with loss 13295.209480716418\n",
            "Iteration: 231 with loss 13291.902340427496\n",
            "Iteration: 232 with loss 13288.64266010989\n",
            "Iteration: 233 with loss 13285.429657879358\n",
            "Iteration: 234 with loss 13282.262566733796\n",
            "Iteration: 235 with loss 13279.14063422501\n",
            "Iteration: 236 with loss 13276.06312213677\n",
            "Iteration: 237 with loss 13273.02930617269\n",
            "Iteration: 238 with loss 13270.038475651945\n",
            "Iteration: 239 with loss 13267.089933211444\n",
            "Iteration: 240 with loss 13264.182994516796\n",
            "Iteration: 241 with loss 13261.316987979764\n",
            "Iteration: 242 with loss 13258.491254483564\n",
            "Iteration: 243 with loss 13255.705147114157\n",
            "Iteration: 244 with loss 13252.958030898317\n",
            "Iteration: 245 with loss 13250.249282548833\n",
            "Iteration: 246 with loss 13247.578290215784\n",
            "Iteration: 247 with loss 13244.944453243024\n",
            "Iteration: 248 with loss 13242.347181931545\n",
            "Iteration: 249 with loss 13239.78589730908\n",
            "Iteration: 250 with loss 13237.260030904112\n",
            "Iteration: 251 with loss 13234.769024525307\n",
            "Iteration: 252 with loss 13232.312330047731\n",
            "Iteration: 253 with loss 13229.8894092028\n",
            "Iteration: 254 with loss 13227.499733374261\n",
            "Iteration: 255 with loss 13225.142783397336\n",
            "Iteration: 256 with loss 13222.818049365867\n",
            "Iteration: 257 with loss 13220.525030439894\n",
            "Iteration: 258 with loss 13218.263234661446\n",
            "Iteration: 259 with loss 13216.032178772562\n",
            "Iteration: 260 with loss 13213.831388037932\n",
            "Iteration: 261 with loss 13211.660396072271\n",
            "Iteration: 262 with loss 13209.518744671579\n",
            "Iteration: 263 with loss 13207.405983647232\n",
            "Iteration: 264 with loss 13205.321670665877\n",
            "Iteration: 265 with loss 13203.265371091456\n",
            "Iteration: 266 with loss 13201.236657831114\n",
            "Iteration: 267 with loss 13199.235111184928\n",
            "Iteration: 268 with loss 13197.260318700008\n",
            "Iteration: 269 with loss 13195.31187502546\n",
            "Iteration: 270 with loss 13193.38938177289\n",
            "Iteration: 271 with loss 13191.492447379502\n",
            "Iteration: 272 with loss 13189.620686973904\n",
            "Iteration: 273 with loss 13187.773722244674\n",
            "Iteration: 274 with loss 13185.95118131296\n",
            "Iteration: 275 with loss 13184.152698606955\n",
            "Iteration: 276 with loss 13182.377914739684\n",
            "Iteration: 277 with loss 13180.626476388985\n",
            "Iteration: 278 with loss 13178.898036181099\n",
            "Iteration: 279 with loss 13177.192252575505\n",
            "Iteration: 280 with loss 13175.508789753603\n",
            "Iteration: 281 with loss 13173.847317508702\n",
            "Iteration: 282 with loss 13172.207511139677\n",
            "Iteration: 283 with loss 13170.589051345432\n",
            "Iteration: 284 with loss 13168.991624123128\n",
            "Iteration: 285 with loss 13167.41492066744\n",
            "Iteration: 286 with loss 13165.858637272953\n",
            "Iteration: 287 with loss 13164.32247523745\n",
            "Iteration: 288 with loss 13162.80614076927\n",
            "Iteration: 289 with loss 13161.30934489412\n",
            "Iteration: 290 with loss 13159.831803366005\n",
            "Iteration: 291 with loss 13158.37323657868\n",
            "Iteration: 292 with loss 13156.933369480344\n",
            "Iteration: 293 with loss 13155.511931488169\n",
            "Iteration: 294 with loss 13154.108656406797\n",
            "Iteration: 295 with loss 13152.723282346915\n",
            "Iteration: 296 with loss 13151.355551646559\n",
            "Iteration: 297 with loss 13150.005210793419\n",
            "Iteration: 298 with loss 13148.672010349039\n",
            "Iteration: 299 with loss 13147.35570487462\n",
            "Iteration: 300 with loss 13146.056052858583\n",
            "Iteration: 301 with loss 13144.772816644841\n",
            "Iteration: 302 with loss 13143.505762363344\n",
            "Iteration: 303 with loss 13142.25465986195\n",
            "Iteration: 304 with loss 13141.01928263934\n",
            "Iteration: 305 with loss 13139.799407779237\n",
            "Iteration: 306 with loss 13138.594815886825\n",
            "Iteration: 307 with loss 13137.405291025541\n",
            "Iteration: 308 with loss 13136.230620654873\n",
            "Iteration: 309 with loss 13135.070595571748\n",
            "Iteration: 310 with loss 13133.92500984906\n",
            "Iteration: 311 with loss 13132.793660779309\n",
            "Iteration: 312 with loss 13131.676348817773\n",
            "Iteration: 313 with loss 13130.572877525743\n",
            "Iteration: 314 with loss 13129.48305351743\n",
            "Iteration: 315 with loss 13128.406686405626\n",
            "Iteration: 316 with loss 13127.34358874911\n",
            "Iteration: 317 with loss 13126.293576001923\n",
            "Iteration: 318 with loss 13125.256466462793\n",
            "Iteration: 319 with loss 13124.232081226039\n",
            "Iteration: 320 with loss 13123.220244132317\n",
            "Iteration: 321 with loss 13122.220781722202\n",
            "Iteration: 322 with loss 13121.233523189467\n",
            "Iteration: 323 with loss 13120.258300334543\n",
            "Iteration: 324 with loss 13119.294947521379\n",
            "Iteration: 325 with loss 13118.343301632538\n",
            "Iteration: 326 with loss 13117.403202026258\n",
            "Iteration: 327 with loss 13116.474490494695\n",
            "Iteration: 328 with loss 13115.55701122182\n",
            "Iteration: 329 with loss 13114.650610744575\n",
            "Iteration: 330 with loss 13113.75513791064\n",
            "Iteration: 331 with loss 13112.870443841557\n",
            "Iteration: 332 with loss 13111.99638189297\n",
            "Iteration: 333 with loss 13111.132807618238\n",
            "Iteration: 334 with loss 13110.279578730717\n",
            "Iteration: 335 with loss 13109.436555067634\n",
            "Iteration: 336 with loss 13108.603598555921\n",
            "Iteration: 337 with loss 13107.780573175223\n",
            "Iteration: 338 with loss 13106.967344926094\n",
            "Iteration: 339 with loss 13106.163781794605\n",
            "Iteration: 340 with loss 13105.369753720412\n",
            "Iteration: 341 with loss 13104.58513256468\n",
            "Iteration: 342 with loss 13103.809792077704\n",
            "Iteration: 343 with loss 13103.043607868369\n",
            "Iteration: 344 with loss 13102.286457373304\n",
            "Iteration: 345 with loss 13101.538219827418\n",
            "Iteration: 346 with loss 13100.798776233934\n",
            "Iteration: 347 with loss 13100.06800933603\n",
            "Iteration: 348 with loss 13099.345803588487\n",
            "Iteration: 349 with loss 13098.632045129018\n",
            "Iteration: 350 with loss 13097.926621752786\n",
            "Iteration: 351 with loss 13097.229422883587\n",
            "Iteration: 352 with loss 13096.540339549027\n",
            "Iteration: 353 with loss 13095.859264353885\n",
            "Iteration: 354 with loss 13095.186091455129\n",
            "Iteration: 355 with loss 13094.520716537012\n",
            "Iteration: 356 with loss 13093.863036786082\n",
            "Iteration: 357 with loss 13093.212950867792\n",
            "Iteration: 358 with loss 13092.570358903184\n",
            "Iteration: 359 with loss 13091.935162444312\n",
            "Iteration: 360 with loss 13091.307264453439\n",
            "Iteration: 361 with loss 13090.686569278714\n",
            "Iteration: 362 with loss 13090.072982633792\n",
            "Iteration: 363 with loss 13089.466411575504\n",
            "Iteration: 364 with loss 13088.866764482489\n",
            "Iteration: 365 with loss 13088.273951035659\n",
            "Iteration: 366 with loss 13087.687882195733\n",
            "Iteration: 367 with loss 13087.108470184881\n",
            "Iteration: 368 with loss 13086.535628466103\n",
            "Iteration: 369 with loss 13085.969271724429\n",
            "Iteration: 370 with loss 13085.409315847082\n",
            "Iteration: 371 with loss 13084.85567790571\n",
            "Iteration: 372 with loss 13084.308276137142\n",
            "Iteration: 373 with loss 13083.76702992591\n",
            "Iteration: 374 with loss 13083.231859786247\n",
            "Iteration: 375 with loss 13082.702687344497\n",
            "Iteration: 376 with loss 13082.179435322321\n",
            "Iteration: 377 with loss 13081.662027519562\n",
            "Iteration: 378 with loss 13081.150388797845\n",
            "Iteration: 379 with loss 13080.644445063996\n",
            "Iteration: 380 with loss 13080.14412325457\n",
            "Iteration: 381 with loss 13079.649351319493\n",
            "Iteration: 382 with loss 13079.160058207317\n",
            "Iteration: 383 with loss 13078.676173849013\n",
            "Iteration: 384 with loss 13078.197629144284\n",
            "Iteration: 385 with loss 13077.72435594577\n",
            "Iteration: 386 with loss 13077.256287045237\n",
            "Iteration: 387 with loss 13076.793356159045\n",
            "Iteration: 388 with loss 13076.335497914495\n",
            "Iteration: 389 with loss 13075.882647835635\n",
            "Iteration: 390 with loss 13075.434742330643\n",
            "Iteration: 391 with loss 13074.99171867745\n",
            "Iteration: 392 with loss 13074.553515011337\n",
            "Iteration: 393 with loss 13074.120070312165\n",
            "Iteration: 394 with loss 13073.69132439143\n",
            "Iteration: 395 with loss 13073.267217879866\n",
            "Iteration: 396 with loss 13072.847692215797\n",
            "Iteration: 397 with loss 13072.432689631923\n",
            "Iteration: 398 with loss 13072.022153145319\n",
            "Iteration: 399 with loss 13071.616026543972\n",
            "Iteration: 400 with loss 13071.21425437661\n",
            "Iteration: 401 with loss 13070.816781940925\n",
            "Iteration: 402 with loss 13070.423555272446\n",
            "Iteration: 403 with loss 13070.034521134217\n",
            "Iteration: 404 with loss 13069.649627005496\n",
            "Iteration: 405 with loss 13069.268821071835\n",
            "Iteration: 406 with loss 13068.89205221372\n",
            "Iteration: 407 with loss 13068.519269997769\n",
            "Iteration: 408 with loss 13068.150424665695\n",
            "Iteration: 409 with loss 13067.785467124595\n",
            "Iteration: 410 with loss 13067.424348937766\n",
            "Iteration: 411 with loss 13067.067022314712\n",
            "Iteration: 412 with loss 13066.713440101697\n",
            "Iteration: 413 with loss 13066.36355577287\n",
            "Iteration: 414 with loss 13066.01732342105\n",
            "Iteration: 415 with loss 13065.674697748378\n",
            "Iteration: 416 with loss 13065.335634058098\n",
            "Iteration: 417 with loss 13065.000088245919\n",
            "Iteration: 418 with loss 13064.668016790278\n",
            "Iteration: 419 with loss 13064.339376746431\n",
            "Iteration: 420 with loss 13064.01412573538\n",
            "Iteration: 421 with loss 13063.692221937192\n",
            "Iteration: 422 with loss 13063.373624083837\n",
            "Iteration: 423 with loss 13063.058291448924\n",
            "Iteration: 424 with loss 13062.746183842686\n",
            "Iteration: 425 with loss 13062.437261601903\n",
            "Iteration: 426 with loss 13062.131485584283\n",
            "Iteration: 427 with loss 13061.82881716007\n",
            "Iteration: 428 with loss 13061.52921820494\n",
            "Iteration: 429 with loss 13061.232651092912\n",
            "Iteration: 430 with loss 13060.939078689495\n",
            "Iteration: 431 with loss 13060.64846434449\n",
            "Iteration: 432 with loss 13060.360771884985\n",
            "Iteration: 433 with loss 13060.075965609216\n",
            "Iteration: 434 with loss 13059.794010279347\n",
            "Iteration: 435 with loss 13059.514871115129\n",
            "Iteration: 436 with loss 13059.238513787843\n",
            "Iteration: 437 with loss 13058.964904413777\n",
            "Iteration: 438 with loss 13058.694009547386\n",
            "Iteration: 439 with loss 13058.425796176236\n",
            "Iteration: 440 with loss 13058.160231713957\n",
            "Iteration: 441 with loss 13057.897283995071\n",
            "Iteration: 442 with loss 13057.636921268677\n",
            "Iteration: 443 with loss 13057.379112193217\n",
            "Iteration: 444 with loss 13057.123825829942\n",
            "Iteration: 445 with loss 13056.871031637991\n",
            "Iteration: 446 with loss 13056.620699468715\n",
            "Iteration: 447 with loss 13056.3727995604\n",
            "Iteration: 448 with loss 13056.127302532628\n",
            "Iteration: 449 with loss 13055.884179380932\n",
            "Iteration: 450 with loss 13055.643401472502\n",
            "Iteration: 451 with loss 13055.404940539644\n",
            "Iteration: 452 with loss 13055.168768676109\n",
            "Iteration: 453 with loss 13054.93485833135\n",
            "Iteration: 454 with loss 13054.703182306042\n",
            "Iteration: 455 with loss 13054.4737137468\n",
            "Iteration: 456 with loss 13054.246426142054\n",
            "Iteration: 457 with loss 13054.021293316819\n",
            "Iteration: 458 with loss 13053.798289428765\n",
            "Iteration: 459 with loss 13053.577388962854\n",
            "Iteration: 460 with loss 13053.358566727586\n",
            "Iteration: 461 with loss 13053.14179785035\n",
            "Iteration: 462 with loss 13052.927057773219\n",
            "Iteration: 463 with loss 13052.71432224798\n",
            "Iteration: 464 with loss 13052.50356733344\n",
            "Iteration: 465 with loss 13052.294769389548\n",
            "Iteration: 466 with loss 13052.08790507477\n",
            "Iteration: 467 with loss 13051.882951340827\n",
            "Iteration: 468 with loss 13051.679885429974\n",
            "Iteration: 469 with loss 13051.478684870199\n",
            "Iteration: 470 with loss 13051.279327471777\n",
            "Iteration: 471 with loss 13051.08179132277\n",
            "Iteration: 472 with loss 13050.886054786959\n",
            "Iteration: 473 with loss 13050.692096498073\n",
            "Iteration: 474 with loss 13050.499895357993\n",
            "Iteration: 475 with loss 13050.309430531895\n",
            "Iteration: 476 with loss 13050.120681445585\n",
            "Iteration: 477 with loss 13049.933627781\n",
            "Iteration: 478 with loss 13049.748249474433\n",
            "Iteration: 479 with loss 13049.564526711214\n",
            "Iteration: 480 with loss 13049.382439924046\n",
            "Iteration: 481 with loss 13049.201969788786\n",
            "Iteration: 482 with loss 13049.023097221028\n",
            "Iteration: 483 with loss 13048.845803374445\n",
            "Iteration: 484 with loss 13048.670069635422\n",
            "Iteration: 485 with loss 13048.495877622117\n",
            "Iteration: 486 with loss 13048.323209179587\n",
            "Iteration: 487 with loss 13048.152046378043\n",
            "Iteration: 488 with loss 13047.982371509495\n",
            "Iteration: 489 with loss 13047.814167084649\n",
            "Iteration: 490 with loss 13047.647415830255\n",
            "Iteration: 491 with loss 13047.482100686446\n",
            "Iteration: 492 with loss 13047.318204802912\n",
            "Iteration: 493 with loss 13047.155711537953\n",
            "Iteration: 494 with loss 13046.994604454218\n",
            "Iteration: 495 with loss 13046.834867316391\n",
            "Iteration: 496 with loss 13046.676484089086\n",
            "Iteration: 497 with loss 13046.519438933674\n",
            "Iteration: 498 with loss 13046.363716206168\n",
            "Iteration: 499 with loss 13046.209300453784\n",
            "Iteration: 500 with loss 13046.05617641398\n",
            "Final train loss: 13046.056176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaSOI8rstNFE"
      },
      "source": [
        "predictions = predict(w,test_inputs)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZ7CeUbEElT"
      },
      "source": [
        "def Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_precision = 0\n",
        "  queries_num = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    \n",
        "    relevant_docs = test_relevant_passages_dict.get(key)[0]\n",
        "    irrelevant_docs = test_relevant_passages_dict.get(key)[1]\n",
        "    if(len(relevant_docs) != 0):\n",
        "      queries_num += 1\n",
        "      ### based on all the candidate passages create a list with tuples (ranking,label)\n",
        "      #print(str(key) + \" \" + str(len(relevant_docs)) + \"  \" + str(len(irrelevant_docs)))\n",
        "      ranking_list = []\n",
        "      for (_,index) in relevant_docs + irrelevant_docs:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      sorted_ranking = sorted_ranking[0:100]\n",
        "      indx = 1\n",
        "      precision = []\n",
        "      relevant_docs_found = 0\n",
        "      for (_,label) in sorted_ranking:\n",
        "        if(label == 1):\n",
        "          #print(\"relevant doc ranked  at \" + str(indx))\n",
        "          relevant_docs_found += 1\n",
        "        precision.append(relevant_docs_found/indx)\n",
        "        indx += 1\n",
        "      query_precision = np.sum(np.array(precision))/len(sorted_ranking)\n",
        "      #print(\"Precision for query \" + str(key) + \" is \" + str(query_precision))\n",
        "\n",
        "      average_precision += query_precision\n",
        "\n",
        "  print(\"Average Precision: \" + str(average_precision/queries_num))   "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04iSGWEukwYr"
      },
      "source": [
        "def Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_ndcg = 0\n",
        "  queries_num = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    relevant_docs = test_relevant_passages_dict.get(key)[0]\n",
        "    irrelevant_docs = test_relevant_passages_dict.get(key)[1]\n",
        "    if(len(relevant_docs) != 0):\n",
        "\n",
        "      queries_num += 1\n",
        "\n",
        "      ## compute IDCG\n",
        "      IDCG = 0\n",
        "      for i in range(len(relevant_docs)):\n",
        "        IDCG += 1/(np.log2((i+1)+1))\n",
        "\n",
        "      ranking_list = []\n",
        "      for (_,index) in relevant_docs + irrelevant_docs:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      sorted_ranking = sorted_ranking[0:100]\n",
        "\n",
        "      ## compute DCG\n",
        "      DCG = 0\n",
        "      for i,(_,label) in enumerate(sorted_ranking):\n",
        "        if(label == 1):\n",
        "          DCG += 1/(np.log2((i+1)+1))\n",
        "\n",
        "      average_ndcg += DCG/IDCG\n",
        "\n",
        "  print(\"Average NDCG: \" + str(average_ndcg/queries_num))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AHaC7J7FDn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48263611-d322-48d7-b9c0-a7450ca73adc"
      },
      "source": [
        "Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Precision: 0.014344570480704916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peCOKw21pBDH",
        "outputId": "1da2a83c-f2e3-49f2-8da3-04f7cb87f42b"
      },
      "source": [
        "Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average NDCG: 0.20504446266349274\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}