{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRDM_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4PihRhE3h6s/vFq1KgVK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apof/Information-Retrieval---Data-Mining-/blob/main/IRDM_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qvy-qmt-Pm",
        "outputId": "dcc2267b-0934-4722-99f3-2276293246a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzlwMzcmb7bw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDu6NhklusF"
      },
      "source": [
        "**Load The Data and Process the with Pandas DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQwDZDKHuH1l"
      },
      "source": [
        "train_data_file = \"drive/MyDrive/Datasets/IRDM/train_data.tsv\"\n",
        "test_data_file = \"drive/MyDrive/Datasets/IRDM/validation_data.tsv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btO5-N2ybl2O",
        "outputId": "554ed08a-2c5d-432c-b9ac-95cfbce858d5"
      },
      "source": [
        "col_names=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data_init=pd.read_csv(train_data_file, sep='\\t', header=None, names=col_names)\n",
        "train_data_df=pd.DataFrame(train_data_init)\n",
        "train_data_df = train_data_df.iloc[1:]\n",
        "\n",
        "test_data_init=pd.read_csv(test_data_file, sep='\\t', header=None, names=col_names)\n",
        "test_data_df=pd.DataFrame(test_data_init)\n",
        "test_data_df = test_data_df.iloc[1:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2182zoCaeOy0"
      },
      "source": [
        "train_data_df[\"relevancy\"] = train_data_df.relevancy.astype(float)\n",
        "test_data_df[\"relevancy\"] = test_data_df.relevancy.astype(float)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWdiR4ehWr5"
      },
      "source": [
        "train_labels = train_data_df['relevancy'].values\n",
        "test_labels = test_data_df['relevancy'].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DclKom1jTXP"
      },
      "source": [
        "train_passages = train_data_df['passage'].values\n",
        "train_queries = train_data_df['query'].values\n",
        "train_pids = train_data_df['pid'].values\n",
        "train_qids = train_data_df['qid'].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAxbJF1mtw8"
      },
      "source": [
        "test_passages = test_data_df['passage'].values\n",
        "test_queries = test_data_df['query'].values\n",
        "test_pids = test_data_df['pid'].values\n",
        "test_qids = test_data_df['qid'].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpukHGbvzKb2"
      },
      "source": [
        "**Create Dictionaries for the Training and Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9soNcNQGzIFn"
      },
      "source": [
        "train_passage_dict = {}\n",
        "train_query_dict = {}\n",
        "for i, _ in enumerate(train_queries):\n",
        "  train_passage_dict[train_pids[i]] = train_passages[i]\n",
        "  train_query_dict[train_qids[i]] = train_queries[i]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJz1sCG433R8"
      },
      "source": [
        "test_passage_dict = {}\n",
        "test_query_dict = {}\n",
        "for i, _ in enumerate(test_queries):\n",
        "  test_passage_dict[test_pids[i]] = test_passages[i]\n",
        "  test_query_dict[test_qids[i]] = test_queries[i]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx4snFI1thV0"
      },
      "source": [
        "**Correct Imbalanced Data with Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nozDa9ttqD"
      },
      "source": [
        "def collect_relevant_pids(pids,qids,relevancy):\n",
        "\n",
        "  relevancy_dict = {}\n",
        "  ## collect for each query all the relevant passages (relevant passages is the minority class)\n",
        "  ## and then sample randomply K negative examples\n",
        "\n",
        "  for i, qid in enumerate(qids):\n",
        "    if(relevancy_dict.get(qid) == None):\n",
        "      if(relevancy[i] == 0):\n",
        "        relevancy_dict[qid] = [[],[(pids[i],i)]]\n",
        "      else:\n",
        "        relevancy_dict[qid] = [[(pids[i],i)],[]]\n",
        "    else:\n",
        "      l = relevancy_dict.get(qid)\n",
        "      if(relevancy[i] == 0):\n",
        "        l[1].append((pids[i],i))\n",
        "      else:\n",
        "        l[0].append((pids[i],i))\n",
        "      relevancy_dict[qid] = l\n",
        "\n",
        "  return relevancy_dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Sb4rSpvP9n"
      },
      "source": [
        "train_relevant_passages_dict = collect_relevant_pids(train_pids,train_qids,train_labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZa3r6YL4hpt"
      },
      "source": [
        "test_relevant_passages_dict = collect_relevant_pids(test_pids,test_qids,test_labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQXi92iZwCKf"
      },
      "source": [
        "def negative_sampling(relevant_passages_dict,passage_dict,query_dict,K):\n",
        "\n",
        "  sampled_passages = []\n",
        "  sampled_queries = []\n",
        "  sampled_labels = []\n",
        "  sampled_qids = []\n",
        "  sampled_pids = []\n",
        "\n",
        "\n",
        "  for key in relevant_passages_dict:\n",
        "    l = relevant_passages_dict.get(key)\n",
        "\n",
        "    number_of_positive_samples = len(l[0])\n",
        "    number_of_negative_passages = len(l[1])\n",
        "\n",
        "    #print(\"query \" + str(key) + \" has \" + str(number_of_positive_samples) + \" positive and \" + str(number_of_negative_passages) + \" negative sample\" )\n",
        "\n",
        "    ## if there are not relevant passages continue\n",
        "    if(number_of_positive_samples != 0):\n",
        "      ## collect all relevant passages\n",
        "      for (pid,_) in l[0]:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(pid))\n",
        "        sampled_labels.append(1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(pid)\n",
        "\n",
        "      negative_examples_to_draw = None\n",
        "      ## if the positive examples for a query are more than the negative one --> select all the negative and add them to the dataset\n",
        "      if(number_of_positive_samples >= number_of_negative_passages):\n",
        "        negative_examples_to_draw = number_of_negative_passages\n",
        "      else:\n",
        "        ## if there are more negative examples than K\n",
        "        if(number_of_negative_passages >= K):\n",
        "          negative_examples_to_draw = K\n",
        "        else:\n",
        "          ## elese collect all the negaative examples\n",
        "          negative_examples_to_draw = number_of_positive_samples\n",
        "\n",
        "      ## select a number of negative samples equal to the number of the positive ones\n",
        "      indexes = random.sample(range(0,number_of_negative_passages), negative_examples_to_draw)\n",
        "\n",
        "      for i in indexes:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(l[1][i][0]))\n",
        "        sampled_labels.append(-1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(l[1][i][0])\n",
        "\n",
        "  return sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sLnaKNvpGDL"
      },
      "source": [
        "## how many negative samples to collect for each positive one\n",
        "K = 5"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEc_zfRh3LLJ"
      },
      "source": [
        "sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids = negative_sampling(train_relevant_passages_dict,train_passage_dict,train_query_dict,K)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp5DvgR8_GZ",
        "outputId": "41d10017-81e4-4842-ec0e-549ab1b73dd7"
      },
      "source": [
        "sampled_labels = np.array(sampled_labels)\n",
        "print(np.where(sampled_labels == 1)[0].shape)\n",
        "print(np.where(sampled_labels == -1)[0].shape)\n",
        "\n",
        "print(sampled_labels.shape[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4797,)\n",
            "(22916,)\n",
            "27713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDdJnfsnlaQ"
      },
      "source": [
        "**Preprocess Data and data Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmEm2R_oL-w",
        "outputId": "d6d43691-d9fa-44ec-9bf9-a8198c1dc009"
      },
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import  WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CC1oGrD75zr"
      },
      "source": [
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4RW-o2n_9n",
        "outputId": "acfa4c11-0c35-4b8e-8c97-3404e041eea6"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-01 18:53:58--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.8.94\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.8.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  72.4MB/s    in 34s     \n",
            "\n",
            "2021-04-01 18:54:33 (46.4 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiZeCynoF9n"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8VdP2q4ogG"
      },
      "source": [
        "def process_data(data,rm_stopwords = True, lemm = True):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\t## remove punctuation\n",
        "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
        "\tprocessed_sentences = []\n",
        "\tfor sentence in data:\n",
        "\t\t## tokenise each sentence\n",
        "\t\ttokenised_sentence = tokenizer.tokenize(sentence)\n",
        "\t\t##convert to lower case\n",
        "\t\tsentence = [w.lower() for w in tokenised_sentence]\n",
        "\t\t##exclude non alphabetic words\n",
        "\t\tonly_alpha_sentence = [word for word in sentence if word.isalpha()]\n",
        "\t\t## remove stop words\n",
        "\t\tif(rm_stopwords == True):\n",
        "\t\t\tfiltered_sentence = [w for w in only_alpha_sentence if not w in stop_words]\n",
        "\t\telse:\n",
        "\t\t\tfiltered_sentence = only_alpha_sentence\n",
        "\t\tlemmatized_sentence = []\n",
        "\t\tif(lemm == True):\n",
        "\t\t\t## stemming\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\t#stemmer = PorterStemmer()\n",
        "\t\t\tfor word in filtered_sentence:\n",
        "\t\t\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "\t\t\t\t#lemmatized_sentence.append(stemmer.stem(word))\n",
        "\t\telse:\n",
        "\t\t\tlemmatized_sentence = filtered_sentence\n",
        "\n",
        "\t\tprocessed_sentences.append(lemmatized_sentence)\n",
        "\treturn processed_sentences"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvZjgioo0Nc"
      },
      "source": [
        "train_passages = process_data(sampled_passages)\n",
        "train_queries = process_data(sampled_queries)\n",
        "test_passages = process_data(test_passages)\n",
        "test_queries = process_data(test_queries)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRqgIdl6nDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9a7341-0141-41e7-edfe-b96e45730bae"
      },
      "source": [
        "print(len(train_passages))\n",
        "print(len(train_queries))\n",
        "\n",
        "print(len(test_passages))\n",
        "print(len(test_queries))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27713\n",
            "27713\n",
            "1103039\n",
            "1103039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlfzlqeoSpbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f9862f-48f9-45b0-dbd1-e98b6becfb7c"
      },
      "source": [
        "print(train_passages[0])\n",
        "print(train_queries[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['disco', 'sensation', 'andy', 'gibb', 'dy', 'age', 'knee', 'buckling', 'good', 'look', 'brother', 'songwriting', 'talent', 'backing', 'year', 'old', 'andy', 'gibb', 'staged', 'unprecedented', 'display', 'youthful', 'pop', 'mastery', 'month', 'following', 'american', 'debut', 'spring']\n",
            "['andy', 'gibb', 'die']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcfd6e3pXAXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd81ed3-d096-4925-ef4d-e388d00a9959"
      },
      "source": [
        "max_l = 0\n",
        "for sentence in train_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in train_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "print(max_l)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPx8SdpoXk-o"
      },
      "source": [
        "def find_unique_words(datasets):\n",
        "\n",
        "  # words to indexes\n",
        "  vocab_words = {}\n",
        "  # indexes to embeddings\n",
        "  vocab_embeddings = {}\n",
        "\n",
        "  unique_words = 0\n",
        "\n",
        "  # for every data set training validation testing\n",
        "  for dataset in datasets:\n",
        "    # for every sentence\n",
        "    for sentence in dataset:\n",
        "      # for each word of the sentence\n",
        "      for word in sentence:\n",
        "        # if this word is not saved on the dictionary\n",
        "        if(vocab_words.get(word)==None):\n",
        "          # if this word exists is the word Word2Vec model\n",
        "          if word in word2vec:\n",
        "            unique_words += 1\n",
        "            vocab_words[word] = unique_words\n",
        "            vocab_embeddings[unique_words] = word2vec.wv[word]\n",
        "\n",
        "  return vocab_words,vocab_embeddings"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwTVq4HaXTBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4407bffc-2f63-40b5-9d27-e26b5d9ebb40"
      },
      "source": [
        "## vocab dict --> key each unique word and value a unique id\n",
        "## embeddings dict --> key the unique id of a word and value the respective embedding\n",
        "vocab_dict,vocab_embeddings_dict = find_unique_words([train_passages,train_queries,test_passages,test_queries])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5el6sTEm1s8o"
      },
      "source": [
        "def get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,sentences):\n",
        "  representations = []\n",
        "  ## for every sentence\n",
        "  for sentence in sentences:\n",
        "    sentence_representation = []\n",
        "    for word in sentence:\n",
        "      ## collect all the word embeddings of the respective words of a sentence\n",
        "      word_index = vocab_dict.get(word)\n",
        "      if(word_index!=None):\n",
        "        word_embedding = vocab_embeddings_dict.get(word_index)\n",
        "        sentence_representation.append(np.array(word_embedding))\n",
        "      else:\n",
        "        sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    if(len(sentence_representation) == 0):\n",
        "      sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    sentence_representation = np.array(sentence_representation)\n",
        "\n",
        "    representations.append(np.mean(sentence_representation,axis = 0))\n",
        "\n",
        "  return np.array(representations)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv20wAJO34tJ"
      },
      "source": [
        "train_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_passages)\n",
        "train_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_queries)\n",
        "\n",
        "test_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_passages)\n",
        "test_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_queries)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTeibggV6cdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad7ee31-2ce2-44e7-fce6-dc70f9e6669c"
      },
      "source": [
        "print(train_passage_represenations.shape)\n",
        "print(train_queries_represenations.shape)\n",
        "\n",
        "print(test_passage_represenations.shape)\n",
        "print(test_queries_represenations.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27713, 300)\n",
            "(27713, 300)\n",
            "(1103039, 300)\n",
            "(1103039, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LKo-hg-bIv"
      },
      "source": [
        "def combine_representations(q_rep,p_rep,flag = 'diff'):\n",
        "  representation = []\n",
        "  for i,p in enumerate(p_rep):\n",
        "    if(flag == 'diff'):\n",
        "      representation.append((p - q_rep[i])**2)\n",
        "    else:\n",
        "      representation.append(np.concatenate((p,q_rep[i]),axis = 0))\n",
        "  return np.array(representation)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb4lB_qq_jvX"
      },
      "source": [
        "train_inputs = combine_representations(train_queries_represenations,train_passage_represenations,flag = 'diff')\n",
        "\n",
        "test_inputs = combine_representations(test_queries_represenations,test_passage_represenations,flag = 'diff')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f76oWE8w2Khr"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI4CKPXZ6yyN"
      },
      "source": [
        "def predict(w,xTr):\n",
        "  return xTr@w"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7J9iGOa2O5E"
      },
      "source": [
        "def logistic(w,xTr,yTr):\n",
        "\n",
        "    n, d = xTr.shape\n",
        "    ## compute the logistic loss\n",
        "    loss = 0\n",
        "    for i in range(0,n):\n",
        "        loss += np.log(1 + np.exp(-yTr[i] * w.dot(xTr[i])))\n",
        "\n",
        "    ## compute the gradient\n",
        "    grad = np.zeros((d))\n",
        "    \n",
        "    for i in range(0,n):\n",
        "        term1 = (1/(1 + np.exp(-yTr[i] * w.dot(xTr[i]))))\n",
        "        term2 = np.exp(-yTr[i] * w.dot(xTr[i]))\n",
        "        term3 = (-yTr[i] * xTr[i])      \n",
        "        grad_term = term1 * term2 * term3       \n",
        "        grad  = np.add(grad,grad_term)\n",
        "                   \n",
        "    return loss, grad"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQ8QLi05KkB"
      },
      "source": [
        "def grad_descent(func,w,alpha,maxiter,tol=1e-02):\n",
        "    \n",
        "    losses = []\n",
        "    eps = 1e-06\n",
        "    G_matrix = np.zeros((w.shape[0],w.shape[0]))\n",
        "    \n",
        "    iter_num = 0\n",
        "    criterion = True\n",
        "    \n",
        "    while(iter_num < maxiter and criterion == True):\n",
        "        iter_num += 1\n",
        "        \n",
        "        loss, gradient = func(w)\n",
        "        print(\"Iteration: \" + str(iter_num) + \" with loss \" + str(loss))        \n",
        "        for i in range(w.shape[0]):\n",
        "            G_matrix[i][i] += gradient[i]**2\n",
        "        for i in range(w.shape[0]):\n",
        "            w[i] -= alpha*gradient[i]\n",
        "        losses.append(loss)\n",
        "        if(np.linalg.norm(gradient) < tol):\n",
        "            criterion = False\n",
        "        \n",
        "    return w, losses"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqmWMIAd5qJD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce003203-c199-4743-fcc9-f4b92fc97937"
      },
      "source": [
        "_, d = train_inputs.shape\n",
        "w, losses = grad_descent(lambda weight: logistic(weight, train_inputs, sampled_labels), np.random.rand(d), 0.0001, 500)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.semilogy(losses, c='r', linestyle='-')\n",
        "plt.xlabel(\"gradient updates\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Adagrad convergence\")\n",
        "print(\"Final train loss: %f\" % losses[-1])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1 with loss 36607.49421408309\n",
            "Iteration: 2 with loss 36011.921711595845\n",
            "Iteration: 3 with loss 35425.39424280761\n",
            "Iteration: 4 with loss 34848.01525270949\n",
            "Iteration: 5 with loss 34279.883888129\n",
            "Iteration: 6 with loss 33721.094716616\n",
            "Iteration: 7 with loss 33171.73745196716\n",
            "Iteration: 8 with loss 32631.896688193396\n",
            "Iteration: 9 with loss 32101.651643704863\n",
            "Iteration: 10 with loss 31581.075917420116\n",
            "Iteration: 11 with loss 31070.237258372963\n",
            "Iteration: 12 with loss 30569.1973502047\n",
            "Iteration: 13 with loss 30078.011611690905\n",
            "Iteration: 14 with loss 29596.729014130728\n",
            "Iteration: 15 with loss 29125.391916036217\n",
            "Iteration: 16 with loss 28664.03591510857\n",
            "Iteration: 17 with loss 28212.68971693646\n",
            "Iteration: 18 with loss 27771.375019253537\n",
            "Iteration: 19 with loss 27340.10640992518\n",
            "Iteration: 20 with loss 26918.89127611128\n",
            "Iteration: 21 with loss 26507.72972132571\n",
            "Iteration: 22 with loss 26106.61448637245\n",
            "Iteration: 23 with loss 25715.530869486305\n",
            "Iteration: 24 with loss 25334.45664042714\n",
            "Iteration: 25 with loss 24963.361942929354\n",
            "Iteration: 26 with loss 24602.209179802583\n",
            "Iteration: 27 with loss 24250.952875269268\n",
            "Iteration: 28 with loss 23909.53950990768\n",
            "Iteration: 29 with loss 23577.90732493112\n",
            "Iteration: 30 with loss 23255.9860946379\n",
            "Iteration: 31 with loss 22943.696868735395\n",
            "Iteration: 32 with loss 22640.951689952853\n",
            "Iteration: 33 with loss 22347.65329681196\n",
            "Iteration: 34 with loss 22063.694826420367\n",
            "Iteration: 35 with loss 21788.959537110906\n",
            "Iteration: 36 with loss 21523.320574651032\n",
            "Iteration: 37 with loss 21266.64080683348\n",
            "Iteration: 38 with loss 21018.772746938044\n",
            "Iteration: 39 with loss 20779.55857367349\n",
            "Iteration: 40 with loss 20548.83023130651\n",
            "Iteration: 41 with loss 20326.409559601932\n",
            "Iteration: 42 with loss 20112.108367366425\n",
            "Iteration: 43 with loss 19905.72834567023\n",
            "Iteration: 44 with loss 19707.060747172374\n",
            "Iteration: 45 with loss 19515.885862510644\n",
            "Iteration: 46 with loss 19331.972497718078\n",
            "Iteration: 47 with loss 19155.07783353259\n",
            "Iteration: 48 with loss 18984.948115122246\n",
            "Iteration: 49 with loss 18821.320486852648\n",
            "Iteration: 50 with loss 18663.925969291726\n",
            "Iteration: 51 with loss 18512.49322056655\n",
            "Iteration: 52 with loss 18366.752509795526\n",
            "Iteration: 53 with loss 18226.439340390596\n",
            "Iteration: 54 with loss 18091.297344696253\n",
            "Iteration: 55 with loss 17961.080308540146\n",
            "Iteration: 56 with loss 17835.553374756593\n",
            "Iteration: 57 with loss 17714.493579339454\n",
            "Iteration: 58 with loss 17597.689902612798\n",
            "Iteration: 59 with loss 17484.943000316944\n",
            "Iteration: 60 with loss 17376.064743200077\n",
            "Iteration: 61 with loss 17270.877655937238\n",
            "Iteration: 62 with loss 17169.214314849556\n",
            "Iteration: 63 with loss 17070.916740893797\n",
            "Iteration: 64 with loss 16975.83580883054\n",
            "Iteration: 65 with loss 16883.83068356975\n",
            "Iteration: 66 with loss 16794.76828867874\n",
            "Iteration: 67 with loss 16708.52280857009\n",
            "Iteration: 68 with loss 16624.975223969544\n",
            "Iteration: 69 with loss 16544.012879287184\n",
            "Iteration: 70 with loss 16465.529080076147\n",
            "Iteration: 71 with loss 16389.422718600603\n",
            "Iteration: 72 with loss 16315.597925549391\n",
            "Iteration: 73 with loss 16243.963746003279\n",
            "Iteration: 74 with loss 16174.433837880717\n",
            "Iteration: 75 with loss 16106.926191209484\n",
            "Iteration: 76 with loss 16041.362866688185\n",
            "Iteration: 77 with loss 15977.669752123813\n",
            "Iteration: 78 with loss 15915.776335431925\n",
            "Iteration: 79 with loss 15855.615492993755\n",
            "Iteration: 80 with loss 15797.123292245036\n",
            "Iteration: 81 with loss 15740.238807466762\n",
            "Iteration: 82 with loss 15684.90394781684\n",
            "Iteration: 83 with loss 15631.06329671771\n",
            "Iteration: 84 with loss 15578.66396177977\n",
            "Iteration: 85 with loss 15527.655434502032\n",
            "Iteration: 86 with loss 15477.989459047676\n",
            "Iteration: 87 with loss 15429.619909446254\n",
            "Iteration: 88 with loss 15382.50267461948\n",
            "Iteration: 89 with loss 15336.59555067679\n",
            "Iteration: 90 with loss 15291.858139966713\n",
            "Iteration: 91 with loss 15248.251756407832\n",
            "Iteration: 92 with loss 15205.739336658795\n",
            "Iteration: 93 with loss 15164.285356723845\n",
            "Iteration: 94 with loss 15123.855753614667\n",
            "Iteration: 95 with loss 15084.417851721368\n",
            "Iteration: 96 with loss 15045.940293572456\n",
            "Iteration: 97 with loss 15008.392974683044\n",
            "Iteration: 98 with loss 14971.74698221829\n",
            "Iteration: 99 with loss 14935.974537213562\n",
            "Iteration: 100 with loss 14901.048940118797\n",
            "Iteration: 101 with loss 14866.944519443026\n",
            "Iteration: 102 with loss 14833.636583301897\n",
            "Iteration: 103 with loss 14801.101373673077\n",
            "Iteration: 104 with loss 14769.316023190933\n",
            "Iteration: 105 with loss 14738.258514314033\n",
            "Iteration: 106 with loss 14707.907640717363\n",
            "Iteration: 107 with loss 14678.242970768291\n",
            "Iteration: 108 with loss 14649.244812955889\n",
            "Iteration: 109 with loss 14620.89418315493\n",
            "Iteration: 110 with loss 14593.172773608068\n",
            "Iteration: 111 with loss 14566.062923526335\n",
            "Iteration: 112 with loss 14539.547591206167\n",
            "Iteration: 113 with loss 14513.610327575469\n",
            "Iteration: 114 with loss 14488.235251080661\n",
            "Iteration: 115 with loss 14463.407023840236\n",
            "Iteration: 116 with loss 14439.11082898473\n",
            "Iteration: 117 with loss 14415.33234912103\n",
            "Iteration: 118 with loss 14392.057745851747\n",
            "Iteration: 119 with loss 14369.273640292298\n",
            "Iteration: 120 with loss 14346.967094526717\n",
            "Iteration: 121 with loss 14325.125593952771\n",
            "Iteration: 122 with loss 14303.737030464523\n",
            "Iteration: 123 with loss 14282.78968642788\n",
            "Iteration: 124 with loss 14262.27221940403\n",
            "Iteration: 125 with loss 14242.173647581307\n",
            "Iteration: 126 with loss 14222.483335878404\n",
            "Iteration: 127 with loss 14203.190982680362\n",
            "Iteration: 128 with loss 14184.286607176979\n",
            "Iteration: 129 with loss 14165.760537267835\n",
            "Iteration: 130 with loss 14147.603398010937\n",
            "Iteration: 131 with loss 14129.806100579122\n",
            "Iteration: 132 with loss 14112.359831703145\n",
            "Iteration: 133 with loss 14095.2560435772\n",
            "Iteration: 134 with loss 14078.486444198485\n",
            "Iteration: 135 with loss 14062.042988125258\n",
            "Iteration: 136 with loss 14045.917867626904\n",
            "Iteration: 137 with loss 14030.103504211593\n",
            "Iteration: 138 with loss 14014.592540508815\n",
            "Iteration: 139 with loss 13999.377832491833\n",
            "Iteration: 140 with loss 13984.452442023767\n",
            "Iteration: 141 with loss 13969.80962970968\n",
            "Iteration: 142 with loss 13955.442848042598\n",
            "Iteration: 143 with loss 13941.345734827788\n",
            "Iteration: 144 with loss 13927.512106872591\n",
            "Iteration: 145 with loss 13913.935953930184\n",
            "Iteration: 146 with loss 13900.61143288413\n",
            "Iteration: 147 with loss 13887.53286216388\n",
            "Iteration: 148 with loss 13874.694716379652\n",
            "Iteration: 149 with loss 13862.091621167901\n",
            "Iteration: 150 with loss 13849.718348235512\n",
            "Iteration: 151 with loss 13837.569810597543\n",
            "Iteration: 152 with loss 13825.64105799669\n",
            "Iteration: 153 with loss 13813.927272495894\n",
            "Iteration: 154 with loss 13802.423764240975\n",
            "Iteration: 155 with loss 13791.12596738125\n",
            "Iteration: 156 with loss 13780.029436142826\n",
            "Iteration: 157 with loss 13769.129841047607\n",
            "Iteration: 158 with loss 13758.422965274327\n",
            "Iteration: 159 with loss 13747.904701150743\n",
            "Iteration: 160 with loss 13737.571046776184\n",
            "Iteration: 161 with loss 13727.41810276521\n",
            "Iteration: 162 with loss 13717.442069110652\n",
            "Iteration: 163 with loss 13707.6392421576\n",
            "Iteration: 164 with loss 13698.006011687014\n",
            "Iteration: 165 with loss 13688.538858101467\n",
            "Iteration: 166 with loss 13679.234349712708\n",
            "Iteration: 167 with loss 13670.089140120657\n",
            "Iteration: 168 with loss 13661.09996568831\n",
            "Iteration: 169 with loss 13652.263643101081\n",
            "Iteration: 170 with loss 13643.577067011553\n",
            "Iteration: 171 with loss 13635.03720776616\n",
            "Iteration: 172 with loss 13626.641109207707\n",
            "Iteration: 173 with loss 13618.385886554546\n",
            "Iteration: 174 with loss 13610.26872434908\n",
            "Iteration: 175 with loss 13602.286874477899\n",
            "Iteration: 176 with loss 13594.43765425623\n",
            "Iteration: 177 with loss 13586.71844457689\n",
            "Iteration: 178 with loss 13579.126688120206\n",
            "Iteration: 179 with loss 13571.659887623955\n",
            "Iteration: 180 with loss 13564.315604207904\n",
            "Iteration: 181 with loss 13557.091455755191\n",
            "Iteration: 182 with loss 13549.985115344916\n",
            "Iteration: 183 with loss 13542.994309735415\n",
            "Iteration: 184 with loss 13536.116817896871\n",
            "Iteration: 185 with loss 13529.35046959006\n",
            "Iteration: 186 with loss 13522.693143990738\n",
            "Iteration: 187 with loss 13516.14276835736\n",
            "Iteration: 188 with loss 13509.697316741092\n",
            "Iteration: 189 with loss 13503.354808735401\n",
            "Iteration: 190 with loss 13497.113308266233\n",
            "Iteration: 191 with loss 13490.970922417457\n",
            "Iteration: 192 with loss 13484.925800294488\n",
            "Iteration: 193 with loss 13478.976131922169\n",
            "Iteration: 194 with loss 13473.120147176605\n",
            "Iteration: 195 with loss 13467.356114748398\n",
            "Iteration: 196 with loss 13461.682341139141\n",
            "Iteration: 197 with loss 13456.097169686895\n",
            "Iteration: 198 with loss 13450.598979621505\n",
            "Iteration: 199 with loss 13445.186185147013\n",
            "Iteration: 200 with loss 13439.857234553554\n",
            "Iteration: 201 with loss 13434.61060935386\n",
            "Iteration: 202 with loss 13429.444823444823\n",
            "Iteration: 203 with loss 13424.3584222959\n",
            "Iteration: 204 with loss 13419.349982158912\n",
            "Iteration: 205 with loss 13414.418109301667\n",
            "Iteration: 206 with loss 13409.561439264615\n",
            "Iteration: 207 with loss 13404.778636137835\n",
            "Iteration: 208 with loss 13400.068391859038\n",
            "Iteration: 209 with loss 13395.429425531553\n",
            "Iteration: 210 with loss 13390.860482762668\n",
            "Iteration: 211 with loss 13386.360335019384\n",
            "Iteration: 212 with loss 13381.92777900312\n",
            "Iteration: 213 with loss 13377.561636041948\n",
            "Iteration: 214 with loss 13373.260751499362\n",
            "Iteration: 215 with loss 13369.023994200585\n",
            "Iteration: 216 with loss 13364.850255873444\n",
            "Iteration: 217 with loss 13360.738450605464\n",
            "Iteration: 218 with loss 13356.687514316085\n",
            "Iteration: 219 with loss 13352.696404243023\n",
            "Iteration: 220 with loss 13348.764098441847\n",
            "Iteration: 221 with loss 13344.889595301453\n",
            "Iteration: 222 with loss 13341.071913070331\n",
            "Iteration: 223 with loss 13337.31008939604\n",
            "Iteration: 224 with loss 13333.60318087881\n",
            "Iteration: 225 with loss 13329.950262634291\n",
            "Iteration: 226 with loss 13326.35042787064\n",
            "Iteration: 227 with loss 13322.802787475155\n",
            "Iteration: 228 with loss 13319.306469611502\n",
            "Iteration: 229 with loss 13315.860619330013\n",
            "Iteration: 230 with loss 13312.464398185366\n",
            "Iteration: 231 with loss 13309.116983865735\n",
            "Iteration: 232 with loss 13305.817569831182\n",
            "Iteration: 233 with loss 13302.565364961769\n",
            "Iteration: 234 with loss 13299.359593214294\n",
            "Iteration: 235 with loss 13296.199493288432\n",
            "Iteration: 236 with loss 13293.084318300715\n",
            "Iteration: 237 with loss 13290.013335467132\n",
            "Iteration: 238 with loss 13286.985825794474\n",
            "Iteration: 239 with loss 13284.001083778892\n",
            "Iteration: 240 with loss 13281.058417111022\n",
            "Iteration: 241 with loss 13278.157146390868\n",
            "Iteration: 242 with loss 13275.296604847983\n",
            "Iteration: 243 with loss 13272.476138069365\n",
            "Iteration: 244 with loss 13269.695103733768\n",
            "Iteration: 245 with loss 13266.95287135331\n",
            "Iteration: 246 with loss 13264.248822020387\n",
            "Iteration: 247 with loss 13261.582348161883\n",
            "Iteration: 248 with loss 13258.95285329829\n",
            "Iteration: 249 with loss 13256.359751810438\n",
            "Iteration: 250 with loss 13253.802468709784\n",
            "Iteration: 251 with loss 13251.280439415797\n",
            "Iteration: 252 with loss 13248.793109538052\n",
            "Iteration: 253 with loss 13246.339934664176\n",
            "Iteration: 254 with loss 13243.92038015192\n",
            "Iteration: 255 with loss 13241.533920927017\n",
            "Iteration: 256 with loss 13239.180041286178\n",
            "Iteration: 257 with loss 13236.85823470293\n",
            "Iteration: 258 with loss 13234.568003640486\n",
            "Iteration: 259 with loss 13232.308859367515\n",
            "Iteration: 260 with loss 13230.080321778183\n",
            "Iteration: 261 with loss 13227.881919217842\n",
            "Iteration: 262 with loss 13225.713188310443\n",
            "Iteration: 263 with loss 13223.57367379289\n",
            "Iteration: 264 with loss 13221.46292835\n",
            "Iteration: 265 with loss 13219.380512456182\n",
            "Iteration: 266 with loss 13217.325994219076\n",
            "Iteration: 267 with loss 13215.29894922741\n",
            "Iteration: 268 with loss 13213.29896040195\n",
            "Iteration: 269 with loss 13211.32561785001\n",
            "Iteration: 270 with loss 13209.37851872403\n",
            "Iteration: 271 with loss 13207.457267081332\n",
            "Iteration: 272 with loss 13205.56147375024\n",
            "Iteration: 273 with loss 13203.690756195732\n",
            "Iteration: 274 with loss 13201.844738390542\n",
            "Iteration: 275 with loss 13200.023050688193\n",
            "Iteration: 276 with loss 13198.225329698873\n",
            "Iteration: 277 with loss 13196.451218169084\n",
            "Iteration: 278 with loss 13194.700364861543\n",
            "Iteration: 279 with loss 13192.972424440486\n",
            "Iteration: 280 with loss 13191.26705735851\n",
            "Iteration: 281 with loss 13189.583929744576\n",
            "Iteration: 282 with loss 13187.92271329654\n",
            "Iteration: 283 with loss 13186.283085174837\n",
            "Iteration: 284 with loss 13184.664727898697\n",
            "Iteration: 285 with loss 13183.067329244712\n",
            "Iteration: 286 with loss 13181.490582147262\n",
            "Iteration: 287 with loss 13179.934184602203\n",
            "Iteration: 288 with loss 13178.397839570582\n",
            "Iteration: 289 with loss 13176.881254886604\n",
            "Iteration: 290 with loss 13175.384143166455\n",
            "Iteration: 291 with loss 13173.90622171868\n",
            "Iteration: 292 with loss 13172.447212457679\n",
            "Iteration: 293 with loss 13171.006841817396\n",
            "Iteration: 294 with loss 13169.584840669355\n",
            "Iteration: 295 with loss 13168.180944239328\n",
            "Iteration: 296 with loss 13166.794892028303\n",
            "Iteration: 297 with loss 13165.426427733075\n",
            "Iteration: 298 with loss 13164.075299171041\n",
            "Iteration: 299 with loss 13162.741258203514\n",
            "Iteration: 300 with loss 13161.424060662717\n",
            "Iteration: 301 with loss 13160.123466280476\n",
            "Iteration: 302 with loss 13158.839238616247\n",
            "Iteration: 303 with loss 13157.571144989275\n",
            "Iteration: 304 with loss 13156.318956409963\n",
            "Iteration: 305 with loss 13155.082447514174\n",
            "Iteration: 306 with loss 13153.861396498156\n",
            "Iteration: 307 with loss 13152.6555850546\n",
            "Iteration: 308 with loss 13151.46479831087\n",
            "Iteration: 309 with loss 13150.288824766983\n",
            "Iteration: 310 with loss 13149.127456237107\n",
            "Iteration: 311 with loss 13147.980487789922\n",
            "Iteration: 312 with loss 13146.847717691318\n",
            "Iteration: 313 with loss 13145.728947348289\n",
            "Iteration: 314 with loss 13144.623981253873\n",
            "Iteration: 315 with loss 13143.532626932725\n",
            "Iteration: 316 with loss 13142.454694888336\n",
            "Iteration: 317 with loss 13141.389998551218\n",
            "Iteration: 318 with loss 13140.338354227293\n",
            "Iteration: 319 with loss 13139.299581049117\n",
            "Iteration: 320 with loss 13138.273500925803\n",
            "Iteration: 321 with loss 13137.259938495892\n",
            "Iteration: 322 with loss 13136.258721079945\n",
            "Iteration: 323 with loss 13135.269678633986\n",
            "Iteration: 324 with loss 13134.292643705596\n",
            "Iteration: 325 with loss 13133.327451387771\n",
            "Iteration: 326 with loss 13132.373939277028\n",
            "Iteration: 327 with loss 13131.431947430157\n",
            "Iteration: 328 with loss 13130.501318321745\n",
            "Iteration: 329 with loss 13129.581896804484\n",
            "Iteration: 330 with loss 13128.673530067656\n",
            "Iteration: 331 with loss 13127.77606759858\n",
            "Iteration: 332 with loss 13126.88936114375\n",
            "Iteration: 333 with loss 13126.013264669917\n",
            "Iteration: 334 with loss 13125.14763432841\n",
            "Iteration: 335 with loss 13124.292328417196\n",
            "Iteration: 336 with loss 13123.44720734574\n",
            "Iteration: 337 with loss 13122.612133600056\n",
            "Iteration: 338 with loss 13121.786971707243\n",
            "Iteration: 339 with loss 13120.971588202547\n",
            "Iteration: 340 with loss 13120.16585159576\n",
            "Iteration: 341 with loss 13119.36963233815\n",
            "Iteration: 342 with loss 13118.582802791454\n",
            "Iteration: 343 with loss 13117.805237195425\n",
            "Iteration: 344 with loss 13117.03681163763\n",
            "Iteration: 345 with loss 13116.277404023196\n",
            "Iteration: 346 with loss 13115.526894044811\n",
            "Iteration: 347 with loss 13114.78516315348\n",
            "Iteration: 348 with loss 13114.052094530165\n",
            "Iteration: 349 with loss 13113.327573057983\n",
            "Iteration: 350 with loss 13112.611485293573\n",
            "Iteration: 351 with loss 13111.903719441047\n",
            "Iteration: 352 with loss 13111.20416532501\n",
            "Iteration: 353 with loss 13110.512714364635\n",
            "Iteration: 354 with loss 13109.82925954749\n",
            "Iteration: 355 with loss 13109.15369540481\n",
            "Iteration: 356 with loss 13108.485917986593\n",
            "Iteration: 357 with loss 13107.825824837853\n",
            "Iteration: 358 with loss 13107.173314973656\n",
            "Iteration: 359 with loss 13106.528288856909\n",
            "Iteration: 360 with loss 13105.890648374623\n",
            "Iteration: 361 with loss 13105.260296815475\n",
            "Iteration: 362 with loss 13104.637138847536\n",
            "Iteration: 363 with loss 13104.021080497176\n",
            "Iteration: 364 with loss 13103.412029126332\n",
            "Iteration: 365 with loss 13102.809893412685\n",
            "Iteration: 366 with loss 13102.214583328365\n",
            "Iteration: 367 with loss 13101.626010119999\n",
            "Iteration: 368 with loss 13101.044086288366\n",
            "Iteration: 369 with loss 13100.468725569392\n",
            "Iteration: 370 with loss 13099.899842913852\n",
            "Iteration: 371 with loss 13099.337354469973\n",
            "Iteration: 372 with loss 13098.781177563367\n",
            "Iteration: 373 with loss 13098.231230679898\n",
            "Iteration: 374 with loss 13097.687433446707\n",
            "Iteration: 375 with loss 13097.149706615734\n",
            "Iteration: 376 with loss 13096.617972045282\n",
            "Iteration: 377 with loss 13096.092152683555\n",
            "Iteration: 378 with loss 13095.57217255216\n",
            "Iteration: 379 with loss 13095.057956728717\n",
            "Iteration: 380 with loss 13094.54943133165\n",
            "Iteration: 381 with loss 13094.046523503917\n",
            "Iteration: 382 with loss 13093.549161397657\n",
            "Iteration: 383 with loss 13093.057274157529\n",
            "Iteration: 384 with loss 13092.570791907701\n",
            "Iteration: 385 with loss 13092.089645735598\n",
            "Iteration: 386 with loss 13091.613767677236\n",
            "Iteration: 387 with loss 13091.14309070388\n",
            "Iteration: 388 with loss 13090.677548706744\n",
            "Iteration: 389 with loss 13090.217076484205\n",
            "Iteration: 390 with loss 13089.761609727357\n",
            "Iteration: 391 with loss 13089.311085007095\n",
            "Iteration: 392 with loss 13088.865439760246\n",
            "Iteration: 393 with loss 13088.42461227759\n",
            "Iteration: 394 with loss 13087.988541690533\n",
            "Iteration: 395 with loss 13087.557167958354\n",
            "Iteration: 396 with loss 13087.130431856374\n",
            "Iteration: 397 with loss 13086.708274963423\n",
            "Iteration: 398 with loss 13086.290639650173\n",
            "Iteration: 399 with loss 13085.87746906739\n",
            "Iteration: 400 with loss 13085.468707134081\n",
            "Iteration: 401 with loss 13085.064298526908\n",
            "Iteration: 402 with loss 13084.66418866759\n",
            "Iteration: 403 with loss 13084.268323713432\n",
            "Iteration: 404 with loss 13083.8766505459\n",
            "Iteration: 405 with loss 13083.48911675995\n",
            "Iteration: 406 with loss 13083.10567065304\n",
            "Iteration: 407 with loss 13082.726261216505\n",
            "Iteration: 408 with loss 13082.350838123413\n",
            "Iteration: 409 with loss 13081.979351719992\n",
            "Iteration: 410 with loss 13081.611753015113\n",
            "Iteration: 411 with loss 13081.247993671033\n",
            "Iteration: 412 with loss 13080.888025993798\n",
            "Iteration: 413 with loss 13080.531802924113\n",
            "Iteration: 414 with loss 13080.17927802748\n",
            "Iteration: 415 with loss 13079.830405486262\n",
            "Iteration: 416 with loss 13079.48514008941\n",
            "Iteration: 417 with loss 13079.14343722509\n",
            "Iteration: 418 with loss 13078.805252870954\n",
            "Iteration: 419 with loss 13078.470543586625\n",
            "Iteration: 420 with loss 13078.13926650449\n",
            "Iteration: 421 with loss 13077.811379322253\n",
            "Iteration: 422 with loss 13077.486840293972\n",
            "Iteration: 423 with loss 13077.165608223275\n",
            "Iteration: 424 with loss 13076.84764245399\n",
            "Iteration: 425 with loss 13076.532902864492\n",
            "Iteration: 426 with loss 13076.221349857846\n",
            "Iteration: 427 with loss 13075.912944356121\n",
            "Iteration: 428 with loss 13075.607647791901\n",
            "Iteration: 429 with loss 13075.3054221019\n",
            "Iteration: 430 with loss 13075.006229719427\n",
            "Iteration: 431 with loss 13074.710033567115\n",
            "Iteration: 432 with loss 13074.416797050542\n",
            "Iteration: 433 with loss 13074.126484051181\n",
            "Iteration: 434 with loss 13073.839058919346\n",
            "Iteration: 435 with loss 13073.554486468722\n",
            "Iteration: 436 with loss 13073.272731968564\n",
            "Iteration: 437 with loss 13072.99376113789\n",
            "Iteration: 438 with loss 13072.717540139394\n",
            "Iteration: 439 with loss 13072.444035573006\n",
            "Iteration: 440 with loss 13072.173214469778\n",
            "Iteration: 441 with loss 13071.90504428597\n",
            "Iteration: 442 with loss 13071.639492897002\n",
            "Iteration: 443 with loss 13071.376528591869\n",
            "Iteration: 444 with loss 13071.11612006735\n",
            "Iteration: 445 with loss 13070.858236421747\n",
            "Iteration: 446 with loss 13070.602847150603\n",
            "Iteration: 447 with loss 13070.349922140003\n",
            "Iteration: 448 with loss 13070.099431661716\n",
            "Iteration: 449 with loss 13069.851346367737\n",
            "Iteration: 450 with loss 13069.605637285495\n",
            "Iteration: 451 with loss 13069.362275811704\n",
            "Iteration: 452 with loss 13069.121233708784\n",
            "Iteration: 453 with loss 13068.882483098027\n",
            "Iteration: 454 with loss 13068.64599645628\n",
            "Iteration: 455 with loss 13068.41174661024\n",
            "Iteration: 456 with loss 13068.179706731966\n",
            "Iteration: 457 with loss 13067.94985033386\n",
            "Iteration: 458 with loss 13067.72215126448\n",
            "Iteration: 459 with loss 13067.496583703753\n",
            "Iteration: 460 with loss 13067.27312215794\n",
            "Iteration: 461 with loss 13067.051741456487\n",
            "Iteration: 462 with loss 13066.83241674639\n",
            "Iteration: 463 with loss 13066.615123488667\n",
            "Iteration: 464 with loss 13066.399837453271\n",
            "Iteration: 465 with loss 13066.186534716006\n",
            "Iteration: 466 with loss 13065.975191653777\n",
            "Iteration: 467 with loss 13065.76578494061\n",
            "Iteration: 468 with loss 13065.558291543452\n",
            "Iteration: 469 with loss 13065.352688718473\n",
            "Iteration: 470 with loss 13065.148954007316\n",
            "Iteration: 471 with loss 13064.947065233197\n",
            "Iteration: 472 with loss 13064.747000496856\n",
            "Iteration: 473 with loss 13064.548738172958\n",
            "Iteration: 474 with loss 13064.352256906612\n",
            "Iteration: 475 with loss 13064.157535609644\n",
            "Iteration: 476 with loss 13063.96455345692\n",
            "Iteration: 477 with loss 13063.77328988338\n",
            "Iteration: 478 with loss 13063.583724579634\n",
            "Iteration: 479 with loss 13063.39583748927\n",
            "Iteration: 480 with loss 13063.209608805151\n",
            "Iteration: 481 with loss 13063.025018967073\n",
            "Iteration: 482 with loss 13062.842048656586\n",
            "Iteration: 483 with loss 13062.660678795795\n",
            "Iteration: 484 with loss 13062.480890542629\n",
            "Iteration: 485 with loss 13062.302665288717\n",
            "Iteration: 486 with loss 13062.12598465622\n",
            "Iteration: 487 with loss 13061.95083049392\n",
            "Iteration: 488 with loss 13061.777184875513\n",
            "Iteration: 489 with loss 13061.605030095554\n",
            "Iteration: 490 with loss 13061.43434866708\n",
            "Iteration: 491 with loss 13061.265123319363\n",
            "Iteration: 492 with loss 13061.097336992929\n",
            "Iteration: 493 with loss 13060.93097283975\n",
            "Iteration: 494 with loss 13060.766014218125\n",
            "Iteration: 495 with loss 13060.60244469118\n",
            "Iteration: 496 with loss 13060.440248023659\n",
            "Iteration: 497 with loss 13060.279408179804\n",
            "Iteration: 498 with loss 13060.119909319874\n",
            "Iteration: 499 with loss 13059.961735798819\n",
            "Iteration: 500 with loss 13059.80487216258\n",
            "Final train loss: 13059.804872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaSOI8rstNFE"
      },
      "source": [
        "predictions = predict(w,test_inputs)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZ7CeUbEElT"
      },
      "source": [
        "def Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_precision = 0\n",
        "  queries_num = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    \n",
        "    relevant_docs = test_relevant_passages_dict.get(key)[0]\n",
        "    irrelevant_docs = test_relevant_passages_dict.get(key)[1]\n",
        "    if(len(relevant_docs) != 0):\n",
        "      queries_num += 1\n",
        "      ### based on all the candidate passages create a list with tuples (ranking,label)\n",
        "      #print(str(key) + \" \" + str(len(relevant_docs)) + \"  \" + str(len(irrelevant_docs)))\n",
        "      ranking_list = []\n",
        "      for (_,index) in relevant_docs + irrelevant_docs:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      sorted_ranking = sorted_ranking[0:100]\n",
        "      indx = 1\n",
        "      precision = []\n",
        "      relevant_docs_found = 0\n",
        "      for (_,label) in sorted_ranking:\n",
        "        if(label == 1):\n",
        "          #print(\"relevant doc ranked  at \" + str(indx))\n",
        "          relevant_docs_found += 1\n",
        "        precision.append(relevant_docs_found/indx)\n",
        "        indx += 1\n",
        "      query_precision = np.sum(np.array(precision))/len(sorted_ranking)\n",
        "      #print(\"Precision for query \" + str(key) + \" is \" + str(query_precision))\n",
        "\n",
        "      average_precision += query_precision\n",
        "\n",
        "  print(\"Average Precision: \" + str(average_precision/queries_num))   "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04iSGWEukwYr"
      },
      "source": [
        "def Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_ndcg = 0\n",
        "  queries_num = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    relevant_docs = test_relevant_passages_dict.get(key)[0]\n",
        "    irrelevant_docs = test_relevant_passages_dict.get(key)[1]\n",
        "    if(len(relevant_docs) != 0):\n",
        "\n",
        "      #print(\"Processing key \" + str(key) + str(len(relevant_docs)) + \" \" + str(len(irrelevant_docs)))\n",
        "\n",
        "      #print(relevant_docs)\n",
        "\n",
        "      #print(irrelevant_docs)\n",
        "\n",
        "      queries_num += 1\n",
        "\n",
        "      ## compute IDCG\n",
        "      IDCG = 0\n",
        "      for i in range(len(relevant_docs)):\n",
        "        #print(i)\n",
        "        IDCG += 1/(np.log2(i+1))\n",
        "\n",
        "      ranking_list = []\n",
        "      for (_,index) in relevant_docs + irrelevant_docs:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      sorted_ranking = sorted_ranking[0:100]\n",
        "\n",
        "      ## compute DCG\n",
        "      DCG = 0\n",
        "      for i,(_,label) in enumerate(sorted_ranking):\n",
        "        if(label == 1):\n",
        "          DCG += 1/(np.log2(i+1))\n",
        "\n",
        "      \n",
        "      #print(DCG)\n",
        "      #print(IDCG)\n",
        "      #print(\"---\")\n",
        "      average_ndcg += DCG/IDCG\n",
        "\n",
        "  print(\"Average NDCG: \" + str(average_ndcg/queries_num))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AHaC7J7FDn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1522de-9baf-4564-e896-929b9d19c360"
      },
      "source": [
        "Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Precision: 0.01432658801713501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peCOKw21pBDH",
        "outputId": "8f221bf0-d8b4-411e-8935-09bc59dbc9ec"
      },
      "source": [
        "Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average NDCG: nan\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}