{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRDM_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM2+B1n+3F0LoPniHuOKGuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apof/Information-Retrieval---Data-Mining-/blob/main/IRDM_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qvy-qmt-Pm",
        "outputId": "43f075a3-2b99-4b53-f640-3da9405cf9b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzlwMzcmb7bw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDu6NhklusF"
      },
      "source": [
        "**Load The Data and Process the with Pandas DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQwDZDKHuH1l"
      },
      "source": [
        "train_data_file = \"drive/MyDrive/Datasets/IRDM/train_data.tsv\"\n",
        "test_data_file = \"drive/MyDrive/Datasets/IRDM/validation_data.tsv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btO5-N2ybl2O",
        "outputId": "69cbe65e-389d-4a25-afcf-b25482087f3b"
      },
      "source": [
        "col_names=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data_init=pd.read_csv(train_data_file, sep='\\t', header=None, names=col_names)\n",
        "train_data_df=pd.DataFrame(train_data_init)\n",
        "train_data_df = train_data_df.iloc[1:]\n",
        "\n",
        "test_data_init=pd.read_csv(test_data_file, sep='\\t', header=None, names=col_names)\n",
        "test_data_df=pd.DataFrame(test_data_init)\n",
        "test_data_df = test_data_df.iloc[1:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2182zoCaeOy0"
      },
      "source": [
        "train_data_df[\"relevancy\"] = train_data_df.relevancy.astype(float)\n",
        "test_data_df[\"relevancy\"] = test_data_df.relevancy.astype(float)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWdiR4ehWr5"
      },
      "source": [
        "train_labels = train_data_df['relevancy'].values\n",
        "test_labels = test_data_df['relevancy'].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DclKom1jTXP"
      },
      "source": [
        "train_passages = train_data_df['passage'].values\n",
        "train_queries = train_data_df['query'].values\n",
        "train_pids = train_data_df['pid'].values\n",
        "train_qids = train_data_df['qid'].values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAxbJF1mtw8"
      },
      "source": [
        "test_passages = test_data_df['passage'].values\n",
        "test_queries = test_data_df['query'].values\n",
        "test_pids = test_data_df['pid'].values\n",
        "test_qids = test_data_df['qid'].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpukHGbvzKb2"
      },
      "source": [
        "**Create Dictionaries for the Training and Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9soNcNQGzIFn"
      },
      "source": [
        "train_passage_dict = {}\n",
        "train_query_dict = {}\n",
        "for i, _ in enumerate(train_queries):\n",
        "  train_passage_dict[train_pids[i]] = train_passages[i]\n",
        "  train_query_dict[train_qids[i]] = train_queries[i]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJz1sCG433R8"
      },
      "source": [
        "test_passage_dict = {}\n",
        "test_query_dict = {}\n",
        "for i, _ in enumerate(test_queries):\n",
        "  test_passage_dict[test_pids[i]] = test_passages[i]\n",
        "  test_query_dict[test_qids[i]] = test_queries[i]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx4snFI1thV0"
      },
      "source": [
        "**Correct Imbalanced Data with Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nozDa9ttqD"
      },
      "source": [
        "def collect_relevant_pids(pids,qids,relevancy):\n",
        "\n",
        "  relevancy_dict = {}\n",
        "  ## collect for each query all the relevant passages (relevant passages is the minority class)\n",
        "  ## and then sample randomply K negative examples\n",
        "\n",
        "  for i, qid in enumerate(qids):\n",
        "    if(relevancy_dict.get(qid) == None):\n",
        "      if(relevancy[i] == 0):\n",
        "        relevancy_dict[qid] = [[],[(pids[i],i)]]\n",
        "      else:\n",
        "        relevancy_dict[qid] = [[(pids[i],i)],[]]\n",
        "    else:\n",
        "      l = relevancy_dict.get(qid)\n",
        "      if(relevancy[i] == 0):\n",
        "        l[1].append((pids[i],i))\n",
        "      else:\n",
        "        l[0].append((pids[i],i))\n",
        "      relevancy_dict[qid] = l\n",
        "\n",
        "  return relevancy_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Sb4rSpvP9n"
      },
      "source": [
        "train_relevant_passages_dict = collect_relevant_pids(train_pids,train_qids,train_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZa3r6YL4hpt"
      },
      "source": [
        "test_relevant_passages_dict = collect_relevant_pids(test_pids,test_qids,test_labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQXi92iZwCKf"
      },
      "source": [
        "def negative_sampling(relevant_passages_dict,passage_dict,query_dict,K):\n",
        "\n",
        "  sampled_passages = []\n",
        "  sampled_queries = []\n",
        "  sampled_labels = []\n",
        "  sampled_qids = []\n",
        "  sampled_pids = []\n",
        "\n",
        "\n",
        "  for key in relevant_passages_dict:\n",
        "    l = relevant_passages_dict.get(key)\n",
        "\n",
        "    number_of_positive_samples = len(l[0])\n",
        "    number_of_negative_passages = len(l[1])\n",
        "\n",
        "    #print(\"query \" + str(key) + \" has \" + str(number_of_positive_samples) + \" positive and \" + str(number_of_negative_passages) + \" negative sample\" )\n",
        "\n",
        "    ## if there are not relevant passages continue\n",
        "    if(number_of_positive_samples != 0):\n",
        "      ## collect all relevant passages\n",
        "      for (pid,_) in l[0]:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(pid))\n",
        "        sampled_labels.append(1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(pid)\n",
        "\n",
        "      negative_examples_to_draw = None\n",
        "      ## if the positive examples for a query are more than the negative one --> select all the negative and add them to the dataset\n",
        "      if(number_of_positive_samples >= number_of_negative_passages):\n",
        "        negative_examples_to_draw = number_of_negative_passages\n",
        "      else:\n",
        "        ## if there are more negative examples than K\n",
        "        if(number_of_negative_passages >= K):\n",
        "          negative_examples_to_draw = K\n",
        "        else:\n",
        "          ## elese collect all the negaative examples\n",
        "          negative_examples_to_draw = number_of_positive_samples\n",
        "\n",
        "      ## select a number of negative samples equal to the number of the positive ones\n",
        "      indexes = random.sample(range(0,number_of_negative_passages), negative_examples_to_draw)\n",
        "\n",
        "      for i in indexes:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(l[1][i][0]))\n",
        "        sampled_labels.append(-1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(l[1][i][0])\n",
        "\n",
        "  return sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sLnaKNvpGDL"
      },
      "source": [
        "## how many negative samples to collect for each positive one\n",
        "K = 5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEc_zfRh3LLJ"
      },
      "source": [
        "sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids = negative_sampling(train_relevant_passages_dict,train_passage_dict,train_query_dict,K)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDdJnfsnlaQ"
      },
      "source": [
        "**Preprocess Data and data Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmEm2R_oL-w",
        "outputId": "c6beec5c-840e-4363-a7b7-ad8c6339451c"
      },
      "source": [
        "import joblib\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import  WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CC1oGrD75zr"
      },
      "source": [
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4RW-o2n_9n",
        "outputId": "9df5a67f-fc2a-4779-813a-38ae9cce8750"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-03 16:52:33--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.101.102\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.101.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiZeCynoF9n"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8VdP2q4ogG"
      },
      "source": [
        "def process_data(data,rm_stopwords = True, lemm = True):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\t## remove punctuation\n",
        "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
        "\tprocessed_sentences = []\n",
        "\tfor sentence in data:\n",
        "\t\t## tokenise each sentence\n",
        "\t\ttokenised_sentence = tokenizer.tokenize(sentence)\n",
        "\t\t##convert to lower case\n",
        "\t\tsentence = [w.lower() for w in tokenised_sentence]\n",
        "\t\t##exclude non alphabetic words\n",
        "\t\tonly_alpha_sentence = [word for word in sentence if word.isalpha()]\n",
        "\t\t## remove stop words\n",
        "\t\tif(rm_stopwords == True):\n",
        "\t\t\tfiltered_sentence = [w for w in only_alpha_sentence if not w in stop_words]\n",
        "\t\telse:\n",
        "\t\t\tfiltered_sentence = only_alpha_sentence\n",
        "\t\tlemmatized_sentence = []\n",
        "\t\tif(lemm == True):\n",
        "\t\t\t## stemming\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\t#stemmer = PorterStemmer()\n",
        "\t\t\tfor word in filtered_sentence:\n",
        "\t\t\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "\t\t\t\t#lemmatized_sentence.append(stemmer.stem(word))\n",
        "\t\telse:\n",
        "\t\t\tlemmatized_sentence = filtered_sentence\n",
        "\n",
        "\t\tprocessed_sentences.append(lemmatized_sentence)\n",
        "\treturn processed_sentences"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvZjgioo0Nc"
      },
      "source": [
        "train_passages = process_data(sampled_passages)\n",
        "train_queries = process_data(sampled_queries)\n",
        "test_passages = process_data(test_passages)\n",
        "test_queries = process_data(test_queries)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRqgIdl6nDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9098fd-4537-4412-8965-b9ad613eb4f8"
      },
      "source": [
        "print(len(train_passages))\n",
        "print(len(train_queries))\n",
        "\n",
        "print(len(test_passages))\n",
        "print(len(test_queries))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27713\n",
            "27713\n",
            "1103039\n",
            "1103039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcfd6e3pXAXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551675eb-ca7e-4de6-b78b-f4351016ab30"
      },
      "source": [
        "max_l = 0\n",
        "for sentence in train_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in train_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "print(max_l)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPx8SdpoXk-o"
      },
      "source": [
        "def find_unique_words(datasets):\n",
        "\n",
        "  # words to indexes\n",
        "  vocab_words = {}\n",
        "  # indexes to embeddings\n",
        "  vocab_embeddings = {}\n",
        "\n",
        "  unique_words = 0\n",
        "\n",
        "  # for every data set training validation testing\n",
        "  for dataset in datasets:\n",
        "    # for every sentence\n",
        "    for sentence in dataset:\n",
        "      # for each word of the sentence\n",
        "      for word in sentence:\n",
        "        # if this word is not saved on the dictionary\n",
        "        if(vocab_words.get(word)==None):\n",
        "          # if this word exists is the word Word2Vec model\n",
        "          if word in word2vec:\n",
        "            unique_words += 1\n",
        "            vocab_words[word] = unique_words\n",
        "            vocab_embeddings[unique_words] = word2vec.wv[word]\n",
        "\n",
        "  return vocab_words,vocab_embeddings"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwTVq4HaXTBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347e3f92-e61f-47d4-cf18-769b270a9bdb"
      },
      "source": [
        "## vocab dict --> key each unique word and value a unique id\n",
        "## embeddings dict --> key the unique id of a word and value the respective embedding\n",
        "vocab_dict,vocab_embeddings_dict = find_unique_words([train_passages,train_queries,test_passages,test_queries])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5el6sTEm1s8o"
      },
      "source": [
        "def get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,sentences):\n",
        "  representations = []\n",
        "  ## for every sentence\n",
        "  for sentence in sentences:\n",
        "    sentence_representation = []\n",
        "    for word in sentence:\n",
        "      ## collect all the word embeddings of the respective words of a sentence\n",
        "      word_index = vocab_dict.get(word)\n",
        "      if(word_index!=None):\n",
        "        word_embedding = vocab_embeddings_dict.get(word_index)\n",
        "        sentence_representation.append(np.array(word_embedding))\n",
        "      else:\n",
        "        sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    if(len(sentence_representation) == 0):\n",
        "      sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    sentence_representation = np.array(sentence_representation)\n",
        "\n",
        "    representations.append(np.mean(sentence_representation,axis = 0))\n",
        "\n",
        "  return np.array(representations)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv20wAJO34tJ"
      },
      "source": [
        "train_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_passages)\n",
        "train_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_queries)\n",
        "\n",
        "test_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_passages)\n",
        "test_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_queries)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTeibggV6cdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efcf9b8-2fa8-4b31-d31f-f38ae8b00f8e"
      },
      "source": [
        "print(train_passage_represenations.shape)\n",
        "print(train_queries_represenations.shape)\n",
        "\n",
        "print(test_passage_represenations.shape)\n",
        "print(test_queries_represenations.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27713, 300)\n",
            "(27713, 300)\n",
            "(1103039, 300)\n",
            "(1103039, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LKo-hg-bIv"
      },
      "source": [
        "def combine_representations(q_rep,p_rep,flag = 'diff'):\n",
        "  representation = []\n",
        "  for i,p in enumerate(p_rep):\n",
        "    if(flag == 'diff'):\n",
        "      representation.append((p - q_rep[i])**2)\n",
        "    else:\n",
        "      representation.append(np.concatenate((p,q_rep[i]),axis = 0))\n",
        "  return np.array(representation)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb4lB_qq_jvX"
      },
      "source": [
        "train_inputs = combine_representations(train_queries_represenations,train_passage_represenations,flag = 'diff')\n",
        "\n",
        "test_inputs = combine_representations(test_queries_represenations,test_passage_represenations,flag = 'diff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f76oWE8w2Khr"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI4CKPXZ6yyN"
      },
      "source": [
        "def predict(w,xTr):\n",
        "  return xTr@w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7J9iGOa2O5E"
      },
      "source": [
        "def logistic(w,xTr,yTr):\n",
        "\n",
        "    n, d = xTr.shape\n",
        "    ## compute the logistic loss\n",
        "    loss = 0\n",
        "    for i in range(0,n):\n",
        "        loss += np.log(1 + np.exp(-yTr[i] * w.dot(xTr[i])))\n",
        "\n",
        "    ## compute the gradient\n",
        "    grad = np.zeros((d))\n",
        "    \n",
        "    for i in range(0,n):\n",
        "        term1 = (1/(1 + np.exp(-yTr[i] * w.dot(xTr[i]))))\n",
        "        term2 = np.exp(-yTr[i] * w.dot(xTr[i]))\n",
        "        term3 = (-yTr[i] * xTr[i])      \n",
        "        grad_term = term1 * term2 * term3       \n",
        "        grad  = np.add(grad,grad_term)\n",
        "                   \n",
        "    return loss, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQ8QLi05KkB"
      },
      "source": [
        "def grad_descent(func,w,alpha,maxiter,tol=1e-02):\n",
        "    \n",
        "    losses = []\n",
        "    eps = 1e-06\n",
        "    G_matrix = np.zeros((w.shape[0],w.shape[0]))\n",
        "    \n",
        "    iter_num = 0\n",
        "    criterion = True\n",
        "    \n",
        "    while(iter_num < maxiter and criterion == True):\n",
        "        iter_num += 1\n",
        "        \n",
        "        loss, gradient = func(w)\n",
        "        print(\"Iteration: \" + str(iter_num) + \" with loss \" + str(loss))\n",
        "        clear_output(wait=True)      \n",
        "        for i in range(w.shape[0]):\n",
        "            G_matrix[i][i] += gradient[i]**2\n",
        "        for i in range(w.shape[0]):\n",
        "            w[i] -= alpha*gradient[i]\n",
        "        losses.append(loss)\n",
        "        if(np.linalg.norm(gradient) < tol):\n",
        "            criterion = False\n",
        "        \n",
        "    return w, losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqmWMIAd5qJD"
      },
      "source": [
        "_, d = train_inputs.shape\n",
        "w, losses = grad_descent(lambda weight: logistic(weight, train_inputs, sampled_labels), np.random.rand(d), 0.0001, 500)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.semilogy(losses, c='r', linestyle='-')\n",
        "plt.xlabel(\"gradient updates\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Adagrad convergence\")\n",
        "print(\"Final train loss: %f\" % losses[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaSOI8rstNFE"
      },
      "source": [
        "predictions = predict(w,test_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--3xlr03ogL0"
      },
      "source": [
        "**Evaluation Metrics Mean Precision and NDCG**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZ7CeUbEElT"
      },
      "source": [
        "def Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_precision = 0\n",
        "  queries_num = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    \n",
        "    relevant_docs = test_relevant_passages_dict.get(key)[0]\n",
        "    irrelevant_docs = test_relevant_passages_dict.get(key)[1]\n",
        "    if(len(relevant_docs) != 0):\n",
        "      queries_num += 1\n",
        "      ### based on all the candidate passages create a list with tuples (ranking,label)\n",
        "      #print(str(key) + \" \" + str(len(relevant_docs)) + \"  \" + str(len(irrelevant_docs)))\n",
        "      ranking_list = []\n",
        "      for (_,index) in relevant_docs + irrelevant_docs:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      sorted_ranking = sorted_ranking[0:100]\n",
        "      indx = 1\n",
        "      precision = []\n",
        "      relevant_docs_found = 0\n",
        "      for (_,label) in sorted_ranking:\n",
        "        if(label == 1):\n",
        "          #print(\"relevant doc ranked  at \" + str(indx))\n",
        "          relevant_docs_found += 1\n",
        "        precision.append(relevant_docs_found/indx)\n",
        "        indx += 1\n",
        "      query_precision = np.sum(np.array(precision))/len(sorted_ranking)\n",
        "      #print(\"Precision for query \" + str(key) + \" is \" + str(query_precision))\n",
        "\n",
        "      average_precision += query_precision\n",
        "\n",
        "  print(\"Average Precision: \" + str(average_precision/queries_num))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04iSGWEukwYr"
      },
      "source": [
        "def Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_ndcg = 0\n",
        "  queries_num = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    relevant_docs = test_relevant_passages_dict.get(key)[0]\n",
        "    irrelevant_docs = test_relevant_passages_dict.get(key)[1]\n",
        "    if(len(relevant_docs) != 0):\n",
        "\n",
        "      queries_num += 1\n",
        "\n",
        "      ## compute IDCG\n",
        "      IDCG = 0\n",
        "      for i in range(len(relevant_docs)):\n",
        "        IDCG += 1/(np.log2((i+1)+1))\n",
        "\n",
        "      ranking_list = []\n",
        "      for (_,index) in relevant_docs + irrelevant_docs:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      sorted_ranking = sorted_ranking[0:100]\n",
        "\n",
        "      ## compute DCG\n",
        "      DCG = 0\n",
        "      for i,(_,label) in enumerate(sorted_ranking):\n",
        "        if(label == 1):\n",
        "          DCG += 1/(np.log2((i+1)+1))\n",
        "\n",
        "      average_ndcg += DCG/IDCG\n",
        "\n",
        "  print(\"Average NDCG: \" + str(average_ndcg/queries_num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AHaC7J7FDn9"
      },
      "source": [
        "Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peCOKw21pBDH"
      },
      "source": [
        "Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXyv8QxIY714"
      },
      "source": [
        "**XGBOOST with LambdaMART**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0CCJy7hhCf"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELq6QnqOa0H0"
      },
      "source": [
        "def collect_group_ids_1(sampled_qids):\n",
        "\n",
        "  number_list = []\n",
        "  helper_dict = {}\n",
        "\n",
        "  for i in range(len(sampled_qids)):\n",
        "    if(helper_dict.get(sampled_qids[i]) == None):\n",
        "      helper_dict[sampled_qids[i]] = 1\n",
        "    else:\n",
        "      v = helper_dict.get(sampled_qids[i]) + 1\n",
        "      helper_dict[sampled_qids[i]] = v\n",
        "\n",
        "  for key in helper_dict:\n",
        "    number_list.append(helper_dict.get(key))\n",
        "      \n",
        "  return number_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EhW90rIccr1"
      },
      "source": [
        "train_groupd_ids = collect_group_ids_1(sampled_qids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qYK8UXScuj2"
      },
      "source": [
        "def collect_group_ids_2(test_relevant_passages_dict):\n",
        "  number_list = []\n",
        "\n",
        "  for key in test_relevant_passages_dict:\n",
        "    passages = test_relevant_passages_dict.get(key)\n",
        "    number_list.append(len(passages[0]) + len(passages[1]))\n",
        "\n",
        "  return number_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZiMLDxodK0d"
      },
      "source": [
        "test_group_ids = collect_group_ids_2(test_relevant_passages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivsg1TunhjPz"
      },
      "source": [
        "model = xgb.XGBRanker(  \n",
        "    tree_method='gpu_hist',\n",
        "    booster='gbtree',\n",
        "    objective='rank:pairwise',\n",
        "    random_state=42, \n",
        "    learning_rate=0.1,\n",
        "    colsample_bytree=0.9, \n",
        "    eta=0.05, \n",
        "    max_depth=4, \n",
        "    n_estimators=500, \n",
        "    subsample=0.5 \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VQN682ihoVc"
      },
      "source": [
        "model = model.fit(train_inputs, sampled_labels, group=train_groupd_ids, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ8Gh-0Dp_YM"
      },
      "source": [
        "def get_XGBoost_predictions(model,test_relevant_passages_dict,test_inputs):\n",
        "\n",
        "  result = []\n",
        "\n",
        "  index = 1\n",
        "  for key in test_relevant_passages_dict:\n",
        "    l = test_relevant_passages_dict.get(key)\n",
        "    group_input = []\n",
        "    indexes = []\n",
        "    for (_,i) in l[0] + l[1]:\n",
        "      indexes.append(i)\n",
        "      group_input.append(test_inputs[i])\n",
        "    group_input = np.array(group_input)\n",
        "    #print(group_input.shape)\n",
        "    preds = model.predict(group_input)\n",
        "    for i in range(len(indexes)):\n",
        "      result.append((indexes[i],preds[i]))\n",
        "\n",
        "  ## reorder the predictions\n",
        "  result = sorted(result, key=lambda tup: tup[0], reverse = False)\n",
        "\n",
        "  final_result = []\n",
        "  for (_,p) in result:\n",
        "    final_result.append(p)\n",
        "\n",
        "  return final_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BWdjHk_rHj0"
      },
      "source": [
        "predictions = get_XGBoost_predictions(model,test_relevant_passages_dict,test_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbviRlqIdf9c"
      },
      "source": [
        "Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akiDlySldtWs"
      },
      "source": [
        "Average_NDCG(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}