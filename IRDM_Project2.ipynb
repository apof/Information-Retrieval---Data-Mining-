{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRDM_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOOxa9dfCuSjnX6f/2AHRYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apof/Information-Retrieval---Data-Mining-/blob/main/IRDM_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qvy-qmt-Pm",
        "outputId": "feda75ba-3e90-47ce-d5c0-e2a70369e36d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzlwMzcmb7bw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDu6NhklusF"
      },
      "source": [
        "**Load The Data and Process the with Pandas DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQwDZDKHuH1l"
      },
      "source": [
        "train_data_file = \"drive/MyDrive/Datasets/IRDM/train_data.tsv\"\n",
        "test_data_file = \"drive/MyDrive/Datasets/IRDM/validation_data.tsv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btO5-N2ybl2O",
        "outputId": "971107ee-4169-4415-eb6b-d3fc3f12940f"
      },
      "source": [
        "col_names=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data_init=pd.read_csv(train_data_file, sep='\\t', header=None, names=col_names)\n",
        "train_data_df=pd.DataFrame(train_data_init)\n",
        "train_data_df = train_data_df.iloc[1:]\n",
        "\n",
        "test_data_init=pd.read_csv(test_data_file, sep='\\t', header=None, names=col_names)\n",
        "test_data_df=pd.DataFrame(test_data_init)\n",
        "test_data_df = test_data_df.iloc[1:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2182zoCaeOy0"
      },
      "source": [
        "train_data_df[\"relevancy\"] = train_data_df.relevancy.astype(float)\n",
        "test_data_df[\"relevancy\"] = test_data_df.relevancy.astype(float)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWdiR4ehWr5"
      },
      "source": [
        "train_labels = train_data_df['relevancy'].values\n",
        "test_labels = test_data_df['relevancy'].values\n",
        "\n",
        "test_labels[test_labels==0] = -1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DclKom1jTXP"
      },
      "source": [
        "train_passages = train_data_df['passage'].values\n",
        "train_queries = train_data_df['query'].values\n",
        "train_pids = train_data_df['pid'].values\n",
        "train_qids = train_data_df['qid'].values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAxbJF1mtw8"
      },
      "source": [
        "test_passages = test_data_df['passage'].values\n",
        "test_queries = test_data_df['query'].values\n",
        "test_pids = test_data_df['pid'].values\n",
        "test_qids = test_data_df['qid'].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpukHGbvzKb2"
      },
      "source": [
        "**Create Dictionaries for the Training and Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9soNcNQGzIFn"
      },
      "source": [
        "train_passage_dict = {}\n",
        "train_query_dict = {}\n",
        "for i, _ in enumerate(train_queries):\n",
        "  train_passage_dict[train_pids[i]] = train_passages[i]\n",
        "  train_query_dict[train_qids[i]] = train_queries[i]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJz1sCG433R8"
      },
      "source": [
        "test_passage_dict = {}\n",
        "test_query_dict = {}\n",
        "for i, _ in enumerate(test_queries):\n",
        "  test_passage_dict[test_pids[i]] = test_passages[i]\n",
        "  test_query_dict[test_qids[i]] = test_queries[i]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx4snFI1thV0"
      },
      "source": [
        "**Correct Imbalanced Data with Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nozDa9ttqD"
      },
      "source": [
        "def collect_relevant_pids(pids,qids,relevancy):\n",
        "\n",
        "  relevancy_dict = {}\n",
        "  ## collect for each query all the relevant passages (relevant passages is the minority class)\n",
        "  ## and then sample randomply K negative examples\n",
        "\n",
        "  for i, qid in enumerate(qids):\n",
        "    if(relevancy_dict.get(qid) == None):\n",
        "      if(relevancy[i] == 0):\n",
        "        relevancy_dict[qid] = [[],[(pids[i],i)]]\n",
        "      else:\n",
        "        relevancy_dict[qid] = [[(pids[i],i)],[]]\n",
        "    else:\n",
        "      l = relevancy_dict.get(qid)\n",
        "      if(relevancy[i] == 0):\n",
        "        l[1].append((pids[i],i))\n",
        "      else:\n",
        "        l[0].append((pids[i],i))\n",
        "      relevancy_dict[qid] = l\n",
        "\n",
        "  return relevancy_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Sb4rSpvP9n"
      },
      "source": [
        "train_relevant_passages_dict = collect_relevant_pids(train_pids,train_qids,train_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZa3r6YL4hpt"
      },
      "source": [
        "test_relevant_passages_dict = collect_relevant_pids(test_pids,test_qids,test_labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQXi92iZwCKf"
      },
      "source": [
        "def negative_sampling(relevant_passages_dict,passage_dict,query_dict,K):\n",
        "\n",
        "  sampled_passages = []\n",
        "  sampled_queries = []\n",
        "  sampled_labels = []\n",
        "  sampled_qids = []\n",
        "  sampled_pids = []\n",
        "\n",
        "\n",
        "  for key in relevant_passages_dict:\n",
        "    l = relevant_passages_dict.get(key)\n",
        "\n",
        "    number_of_positive_samples = len(l[0])\n",
        "    number_of_negative_passages = len(l[1])\n",
        "\n",
        "    #print(\"query \" + str(key) + \" has \" + str(number_of_positive_samples) + \" positive and \" + str(number_of_negative_passages) + \" negative sample\" )\n",
        "\n",
        "    ## if there are not relevant passages continue\n",
        "    if(number_of_positive_samples != 0):\n",
        "      ## collect all relevant passages\n",
        "      for (pid,_) in l[0]:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(pid))\n",
        "        sampled_labels.append(1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(pid)\n",
        "\n",
        "      negative_examples_to_draw = None\n",
        "      ## if the positive examples for a query are more than the negative one --> select all the negative and add them to the dataset\n",
        "      if(number_of_positive_samples >= number_of_negative_passages):\n",
        "        negative_examples_to_draw = number_of_negative_passages\n",
        "      else:\n",
        "        ## if there are more negative examples than K\n",
        "        if(number_of_negative_passages >= K):\n",
        "          negative_examples_to_draw = K\n",
        "        else:\n",
        "          ## elese collect all the negaative examples\n",
        "          negative_examples_to_draw = number_of_positive_samples\n",
        "\n",
        "      ## select a number of negative samples equal to the number of the positive ones\n",
        "      indexes = random.sample(range(0,number_of_negative_passages), negative_examples_to_draw)\n",
        "\n",
        "      for i in indexes:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(l[1][i][0]))\n",
        "        sampled_labels.append(-1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(l[1][i][0])\n",
        "\n",
        "  return sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEc_zfRh3LLJ"
      },
      "source": [
        "sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids = negative_sampling(train_relevant_passages_dict,train_passage_dict,train_query_dict,5)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp5DvgR8_GZ",
        "outputId": "87ebedf4-3023-46af-eb81-e23ae46687f1"
      },
      "source": [
        "sampled_labels = np.array(sampled_labels)\n",
        "print(np.where(sampled_labels == 1)[0].shape)\n",
        "print(np.where(sampled_labels == -1)[0].shape)\n",
        "\n",
        "print(sampled_labels.shape[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4797,)\n",
            "(22916,)\n",
            "27713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHT6QkHS9Z59",
        "outputId": "5f7a5d6d-19ae-41c3-c180-0271b6d8829f"
      },
      "source": [
        "N = 13\n",
        "print(sampled_passages[0:N])\n",
        "print(sampled_queries[0:N])\n",
        "print(sampled_labels[0:N])\n",
        "print(sampled_pids[0:N])\n",
        "print(sampled_qids[0:N])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Disco sensation Andy Gibb dies at the age of 30. With his knee-buckling good looks and his brothers' songwriting talents backing him up, 19-year-old Andy Gibb staged an unprecedented display of youthful pop mastery in the 12 months following his American debut in the spring of 1977.\", \"This song is by Andy Williams and appears…. 1  on the album Love Story (1971) 2  on the compilation album Andy Williams' Greatest Hits Vol. 2 (1973) 3  on the album The Essence Of Andy Williams (1993)  on the compilation album 16 Most Requested Songs: Encore! (1995)\", \"Andy is a widower and father to one young son, Opie . In the backdoor pilot episode from The Danny Thomas Show, viewers learn Andy lost Opie's mother when the boy was the least little speck of a baby. Andy's Aunt Bee acts as his live-in housekeeper and as surrogate mother/grandmother to Opie.\", 'Andy McCarthy’s Timeline of Obama’s Trump Surveillance. Mar 6, 2017. RUSH: Andy McCarthy wrote a piece on Saturday: “The Obama Camp’s Disingenuous Denials on FISA Surveillance of Trump.” Now, Andy’s been writing about this shall we say “scandal,” uncovered scandal, for months.', 'According to Oppenheimer, Martha was dead set on marrying upward in social class, and she did. Her husband Andy’s father was a part of the New York Stock Exchange and Andy was a Yale University law student, so they had plenty of money. Before they were even married Andy noticed problems with Martha. One of which was she wasn’t pleased with the engagement ring that he bought her and demanded he return it and buy her one that was bigger, which he did. Then when Andy graduated from law school in 1964, they took a trip to Europe.', 'The contents of this blog, Both poetry and photography have all been registered to Andy Chih Photography and are protected under copyright laws.', 'Euless, Texas. Euless (/ˈjuːlɪs/ YOO-liss) is a city in Tarrant County, Texas, United States, and a suburb of Fort Worth. Euless is part of the Mid-Cities between Dallas and Fort Worth. The population was 51,277 at the 2010 census.', 'Graford, Texas. Graford is a town in Palo Pinto County, Texas, United States. The population was 584 at the 2010 census.', 'Graham, Texas. Graham is a city in north central Texas. It is the county seat of Young County, and as of the 2010 Census had a population of 8,903.', 'Current El Paso County, Texas Population, Demographics and stats in 2016, 2017. Learn more by clicking the Google or Facebook button below. Population Demographics for El Paso County, Texas in 2016 and 2017. Other Counties and Cities in Texas.', 'Fort Bend County Court House in 1948. Fort Bend County is a county in the U.S. state of Texas. As of the 2010 census, its population was 585,375, making it the tenth-most populous county in Texas.', 'Royse City is a city in Rockwall County in the U.S. state of Texas.', 'Take some measurements of your driveway and create your own concrete driveway cost calculator. Be sure to include the garage floor if you’re having that replaced. For example, a driveway 14 feet wide and 50 feet long (14×50) contains 700 square feet. Multiplied by the cost of concrete, you’re total concrete driveway cost will be $3,500-$5,600. If you are interested in knowing how much concrete you will use, here’s how to do that.']\n",
            "['did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'how much does it cost for a concrete driveway']\n",
            "[ 1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1]\n",
            "['2256696', '2505680', '2790942', '2880636', '1201611', '1250782', '1091839', '1593285', '1099973', '2881978', '2265199', '2684673', '2603240']\n",
            "['143293', '143293', '143293', '143293', '143293', '143293', '182081', '182081', '182081', '182081', '182081', '182081', '314307']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDdJnfsnlaQ"
      },
      "source": [
        "**Preprocess Data and data Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmEm2R_oL-w",
        "outputId": "5ce872a0-b7c8-4647-edac-6a1c97e70a8e"
      },
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import  WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CC1oGrD75zr"
      },
      "source": [
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4RW-o2n_9n",
        "outputId": "3daa4137-1da5-4718-eabc-906501abf82c"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-01 12:31:10--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.83.222\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.83.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiZeCynoF9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "2e8360e1-c93b-48ac-8867-fde7b9858176"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3c3b22ff7dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/root/input/GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8VdP2q4ogG"
      },
      "source": [
        "def process_data(data,rm_stopwords = True, lemm = True):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\t## remove punctuation\n",
        "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
        "\tprocessed_sentences = []\n",
        "\tfor sentence in data:\n",
        "\t\t## tokenise each sentence\n",
        "\t\ttokenised_sentence = tokenizer.tokenize(sentence)\n",
        "\t\t##convert to lower case\n",
        "\t\tsentence = [w.lower() for w in tokenised_sentence]\n",
        "\t\t##exclude non alphabetic words\n",
        "\t\tonly_alpha_sentence = [word for word in sentence if word.isalpha()]\n",
        "\t\t## remove stop words\n",
        "\t\tif(rm_stopwords == True):\n",
        "\t\t\tfiltered_sentence = [w for w in only_alpha_sentence if not w in stop_words]\n",
        "\t\telse:\n",
        "\t\t\tfiltered_sentence = only_alpha_sentence\n",
        "\t\tlemmatized_sentence = []\n",
        "\t\tif(lemm == True):\n",
        "\t\t\t## stemming\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\t#stemmer = PorterStemmer()\n",
        "\t\t\tfor word in filtered_sentence:\n",
        "\t\t\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "\t\t\t\t#lemmatized_sentence.append(stemmer.stem(word))\n",
        "\t\telse:\n",
        "\t\t\tlemmatized_sentence = filtered_sentence\n",
        "\n",
        "\t\tprocessed_sentences.append(lemmatized_sentence)\n",
        "\treturn processed_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvZjgioo0Nc"
      },
      "source": [
        "train_passages = process_data(sampled_passages)\n",
        "train_queries = process_data(sampled_queries)\n",
        "test_passages = process_data(test_passages)\n",
        "test_queries = process_data(test_queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cRqgIdl6nDW"
      },
      "source": [
        "print(len(train_passages))\n",
        "print(len(train_queries))\n",
        "\n",
        "print(len(test_passages))\n",
        "print(len(test_queries))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlfzlqeoSpbW"
      },
      "source": [
        "print(train_passages[0])\n",
        "print(train_queries[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcfd6e3pXAXO"
      },
      "source": [
        "max_l = 0\n",
        "for sentence in train_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in train_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "for sentence in test_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "print(max_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPx8SdpoXk-o"
      },
      "source": [
        "def find_unique_words(datasets):\n",
        "\n",
        "  # words to indexes\n",
        "  vocab_words = {}\n",
        "  # indexes to embeddings\n",
        "  vocab_embeddings = {}\n",
        "\n",
        "  unique_words = 0\n",
        "\n",
        "  # for every data set training validation testing\n",
        "  for dataset in datasets:\n",
        "    # for every sentence\n",
        "    for sentence in dataset:\n",
        "      # for each word of the sentence\n",
        "      for word in sentence:\n",
        "        # if this word is not saved on the dictionary\n",
        "        if(vocab_words.get(word)==None):\n",
        "          # if this word exists is the word Word2Vec model\n",
        "          if word in word2vec:\n",
        "            unique_words += 1\n",
        "            vocab_words[word] = unique_words\n",
        "            vocab_embeddings[unique_words] = word2vec.wv[word]\n",
        "\n",
        "  return vocab_words,vocab_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwTVq4HaXTBX"
      },
      "source": [
        "## vocab dict --> key each unique word and value a unique id\n",
        "## embeddings dict --> key the unique id of a word and value the respective embedding\n",
        "vocab_dict,vocab_embeddings_dict = find_unique_words([train_passages,train_queries,test_passages,test_queries])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5el6sTEm1s8o"
      },
      "source": [
        "def get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,sentences):\n",
        "  representations = []\n",
        "  ## for every sentence\n",
        "  for sentence in sentences:\n",
        "    sentence_representation = []\n",
        "    for word in sentence:\n",
        "      ## collect all the word embeddings of the respective words of a sentence\n",
        "      word_index = vocab_dict.get(word)\n",
        "      if(word_index!=None):\n",
        "        word_embedding = vocab_embeddings_dict.get(word_index)\n",
        "        sentence_representation.append(np.array(word_embedding))\n",
        "      else:\n",
        "        sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    if(len(sentence_representation) == 0):\n",
        "      sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    sentence_representation = np.array(sentence_representation)\n",
        "\n",
        "    representations.append(np.mean(sentence_representation,axis = 0))\n",
        "\n",
        "  return np.array(representations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv20wAJO34tJ"
      },
      "source": [
        "train_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_passages)\n",
        "train_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_queries)\n",
        "\n",
        "test_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_passages)\n",
        "test_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,test_queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTeibggV6cdi"
      },
      "source": [
        "print(train_passage_represenations.shape)\n",
        "print(train_queries_represenations.shape)\n",
        "\n",
        "print(test_passage_represenations.shape)\n",
        "print(test_queries_represenations.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LKo-hg-bIv"
      },
      "source": [
        "def combine_representations(q_rep,p_rep,flag = 'diff'):\n",
        "  representation = []\n",
        "  for i,p in enumerate(p_rep):\n",
        "    if(flag == 'diff'):\n",
        "      representation.append((p - q_rep[i])**2)\n",
        "    else:\n",
        "      representation.append(np.concatenate((p,q_rep[i]),axis = 0))\n",
        "  return np.array(representation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb4lB_qq_jvX"
      },
      "source": [
        "train_inputs = combine_representations(train_queries_represenations,train_passage_represenations,flag = 'diff')\n",
        "\n",
        "test_inputs = combine_representations(test_queries_represenations,test_passage_represenations,flag = 'diff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f76oWE8w2Khr"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI4CKPXZ6yyN"
      },
      "source": [
        "def predict(w,xTr):\n",
        "  return xTr@w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7J9iGOa2O5E"
      },
      "source": [
        "def logistic(w,xTr,yTr):\n",
        "\n",
        "    n, d = xTr.shape\n",
        "    ## compute the logistic loss\n",
        "    loss = 0\n",
        "    for i in range(0,n):\n",
        "        loss += np.log(1 + np.exp(-yTr[i] * w.dot(xTr[i])))\n",
        "\n",
        "    ## compute the gradient\n",
        "    grad = np.zeros((d))\n",
        "    \n",
        "    for i in range(0,n):\n",
        "        term1 = (1/(1 + np.exp(-yTr[i] * w.dot(xTr[i]))))\n",
        "        term2 = np.exp(-yTr[i] * w.dot(xTr[i]))\n",
        "        term3 = (-yTr[i] * xTr[i])      \n",
        "        grad_term = term1 * term2 * term3       \n",
        "        grad  = np.add(grad,grad_term)\n",
        "                   \n",
        "    return loss, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQ8QLi05KkB"
      },
      "source": [
        "def grad_descent(func,w,alpha,maxiter,tol=1e-02):\n",
        "    \n",
        "    losses = []\n",
        "    eps = 1e-06\n",
        "    G_matrix = np.zeros((w.shape[0],w.shape[0]))\n",
        "    \n",
        "    iter_num = 0\n",
        "    criterion = True\n",
        "    \n",
        "    while(iter_num < maxiter and criterion == True):\n",
        "        iter_num += 1\n",
        "        \n",
        "        loss, gradient = func(w)\n",
        "        #print(\"Iteration: \" + str(iter_num) + \" with loss \" + str(loss))        \n",
        "        for i in range(w.shape[0]):\n",
        "            G_matrix[i][i] += gradient[i]**2\n",
        "        for i in range(w.shape[0]):\n",
        "            w[i] -= alpha*gradient[i]\n",
        "        losses.append(loss)\n",
        "        if(np.linalg.norm(gradient) < tol):\n",
        "            criterion = False\n",
        "        \n",
        "    return w, losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqmWMIAd5qJD"
      },
      "source": [
        "_, d = train_inputs.shape\n",
        "w, losses = grad_descent(lambda weight: logistic(weight, train_inputs, sampled_labels), np.random.rand(d), 0.0001, 500)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.semilogy(losses, c='r', linestyle='-')\n",
        "plt.xlabel(\"gradient updates\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Adagrad convergence\")\n",
        "print(\"Final train loss: %f\" % losses[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpXiJEJI7DMX"
      },
      "source": [
        "predictions = predict(w,test_inputs)\n",
        "preds = np.sign(predictions)\n",
        "accuracy = (test_labels == preds).sum() / preds.shape[0]\n",
        "TP = ((preds == 1) & (test_labels == 1)).sum()\n",
        "FP = ((preds == 1) & (test_labels == -1)).sum()\n",
        "precision = TP / (TP+FP)\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "print(\"Precision: \" + str(precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZ7CeUbEElT"
      },
      "source": [
        "def Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict):\n",
        "\n",
        "  average_precision = 0\n",
        "\n",
        "  for key in test_query_dict:\n",
        "    \n",
        "    relevant_docs = len(test_relevant_passages_dict.get(key)[0])\n",
        "    irrelevant_docs = len(test_relevant_passages_dict.get(key)[1])\n",
        "    if(relevant_docs != 0):\n",
        "      ### based on all the candidate passages create a list with tuples (ranking,label)\n",
        "      #print(str(key) + \" \" + str(relevant_docs) + \"  \" + str(irrelevant_docs))\n",
        "      positive_list = test_relevant_passages_dict.get(key)[0]\n",
        "      negative_list = test_relevant_passages_dict.get(key)[1]\n",
        "      ranking_list = []\n",
        "      for (_,index) in positive_list + negative_list:\n",
        "        ranking_list.append((predictions[index],test_labels[index]))\n",
        "      ## get the top 100 docs\n",
        "      ranking_list = ranking_list[0:100]\n",
        "      sorted_ranking = sorted(ranking_list, key=lambda tup: tup[0], reverse = True)\n",
        "      indx = 1\n",
        "      precision = []\n",
        "      relevant_docs_found = 0\n",
        "      for (rank,label) in sorted_ranking:\n",
        "        if(label == 1):\n",
        "          #print(\"relevant doc ranked  at \" + str(indx))\n",
        "          relevant_docs_found += 1\n",
        "        precision.append(relevant_docs_found/indx)\n",
        "        indx += 1\n",
        "      query_precision = np.sum(np.array(precision))/len(sorted_ranking)\n",
        "      #print(\"Precision for query \" + str(key) + \" is \" + str(query_precision))\n",
        "\n",
        "      average_precision += query_precision\n",
        "\n",
        "  print(\"Average Precision: \" + str(average_precision/len(test_query_dict)))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AHaC7J7FDn9"
      },
      "source": [
        "Average_Precision(predictions,test_labels,test_query_dict,test_relevant_passages_dict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}