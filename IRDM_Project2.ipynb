{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRDM_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPGLEb2u8JV7d0nzJzZ+D/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apof/Information-Retrieval---Data-Mining-/blob/main/IRDM_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qvy-qmt-Pm",
        "outputId": "650d6517-0238-4f65-888d-40350c056143"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzlwMzcmb7bw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDu6NhklusF"
      },
      "source": [
        "**Load The Data and Process the with Pandas DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQwDZDKHuH1l"
      },
      "source": [
        "train_data_file = \"drive/MyDrive/Datasets/IRDM/train_data.tsv\"\n",
        "test_data_file = \"drive/MyDrive/Datasets/IRDM/validation_data.tsv\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btO5-N2ybl2O",
        "outputId": "f1e000f8-335b-472e-974a-fab335ca2fee"
      },
      "source": [
        "col_names=['qid','pid','query','passage','relevancy']\n",
        "\n",
        "train_data_init=pd.read_csv(train_data_file, sep='\\t', header=None, names=col_names)\n",
        "train_data_df=pd.DataFrame(train_data_init)\n",
        "train_data_df = train_data_df.iloc[1:]\n",
        "\n",
        "test_data_init=pd.read_csv(test_data_file, sep='\\t', header=None, names=col_names)\n",
        "test_data_df=pd.DataFrame(test_data_init)\n",
        "test_data_df = test_data_df.iloc[1:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2182zoCaeOy0"
      },
      "source": [
        "train_data_df[\"relevancy\"] = train_data_df.relevancy.astype(float)\n",
        "test_data_df[\"relevancy\"] = test_data_df.relevancy.astype(float)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWdiR4ehWr5"
      },
      "source": [
        "train_labels = train_data_df['relevancy'].values\n",
        "test_labels = test_data_df['relevancy'].values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DclKom1jTXP"
      },
      "source": [
        "train_passages = train_data_df['passage'].values\n",
        "train_queries = train_data_df['query'].values\n",
        "train_pids = train_data_df['pid'].values\n",
        "train_qids = train_data_df['qid'].values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAxbJF1mtw8"
      },
      "source": [
        "test_passages = test_data_df['passage'].values\n",
        "test_queries = test_data_df['query'].values\n",
        "test_pids = test_data_df['pid'].values\n",
        "test_qids = test_data_df['qid'].values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpukHGbvzKb2"
      },
      "source": [
        "**Create Dictionaries for the Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9soNcNQGzIFn"
      },
      "source": [
        "passage_dict = {}\n",
        "query_dict = {}\n",
        "for i, _ in enumerate(train_queries):\n",
        "  passage_dict[train_pids[i]] = train_passages[i]\n",
        "  query_dict[train_qids[i]] = train_queries[i]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx4snFI1thV0"
      },
      "source": [
        "**Correct Imbalanced Data with Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-nozDa9ttqD"
      },
      "source": [
        "def collect_relevant_pids(pids,qids,relevancy):\n",
        "\n",
        "  relevancy_dict = {}\n",
        "  ## collect for each query all the relevant passages (relevant passages is the minority class)\n",
        "  ## and then sample randomply K negative examples\n",
        "\n",
        "  for i, qid in enumerate(qids):\n",
        "    if(relevancy_dict.get(qid) == None):\n",
        "      if(relevancy[i] == 0):\n",
        "        relevancy_dict[qid] = [[],[pids[i]]]\n",
        "      else:\n",
        "        relevancy_dict[qid] = [[pids[i]],[]]\n",
        "    else:\n",
        "      l = relevancy_dict.get(qid)\n",
        "      if(relevancy[i] == 0):\n",
        "        l[1].append(pids[i])\n",
        "      else:\n",
        "        l[0].append(pids[i])\n",
        "      relevancy_dict[qid] = l\n",
        "\n",
        "  return relevancy_dict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Sb4rSpvP9n"
      },
      "source": [
        "relevant_passages_dict = collect_relevant_pids(train_pids,train_qids,train_labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQXi92iZwCKf"
      },
      "source": [
        "def negative_sampling(relevant_passages_dict,passage_dict,query_dict,K):\n",
        "\n",
        "  sampled_passages = []\n",
        "  sampled_queries = []\n",
        "  sampled_labels = []\n",
        "  sampled_qids = []\n",
        "  sampled_pids = []\n",
        "\n",
        "\n",
        "  for key in relevant_passages_dict:\n",
        "    l = relevant_passages_dict.get(key)\n",
        "\n",
        "    number_of_positive_samples = len(l[0])\n",
        "    number_of_negative_passages = len(l[1])\n",
        "\n",
        "    #print(\"query \" + str(key) + \" has \" + str(number_of_positive_samples) + \" positive and \" + str(number_of_negative_passages) + \" negative sample\" )\n",
        "\n",
        "    ## if there are not relevant passages continue\n",
        "    if(number_of_positive_samples != 0):\n",
        "      ## collect all relevant passages\n",
        "      for pid in l[0]:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(pid))\n",
        "        sampled_labels.append(1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(pid)\n",
        "\n",
        "      negative_examples_to_draw = None\n",
        "      ## if the positive examples for a query are more than the negative one --> select all the negative and add them to the dataset\n",
        "      if(number_of_positive_samples >= number_of_negative_passages):\n",
        "        negative_examples_to_draw = number_of_negative_passages\n",
        "      else:\n",
        "        ## if there are more negative examples than K\n",
        "        if(number_of_negative_passages >= K):\n",
        "          negative_examples_to_draw = K\n",
        "        else:\n",
        "          ## elese collect all the negaative examples\n",
        "          negative_examples_to_draw = number_of_positive_samples\n",
        "\n",
        "      ## select a number of negative samples equal to the number of the positive ones\n",
        "      indexes = random.sample(range(0,number_of_negative_passages), negative_examples_to_draw)\n",
        "\n",
        "      for i in indexes:\n",
        "        sampled_queries.append(query_dict.get(key))\n",
        "        sampled_passages.append(passage_dict.get(l[1][i]))\n",
        "        sampled_labels.append(-1)\n",
        "        sampled_qids.append(key)\n",
        "        sampled_pids.append(l[1][i])\n",
        "\n",
        "  return sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEc_zfRh3LLJ"
      },
      "source": [
        "sampled_passages, sampled_queries, sampled_labels, sampled_pids, sampled_qids = negative_sampling(relevant_passages_dict,passage_dict,query_dict,5)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sp5DvgR8_GZ",
        "outputId": "a3bdd086-7c3e-44b8-d6a7-83210ab4322b"
      },
      "source": [
        "sampled_labels = np.array(sampled_labels)\n",
        "print(np.where(sampled_labels == 1)[0].shape)\n",
        "print(np.where(sampled_labels == -1)[0].shape)\n",
        "\n",
        "print(sampled_labels.shape[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4797,)\n",
            "(22916,)\n",
            "27713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHT6QkHS9Z59",
        "outputId": "055bf0dc-156a-46de-e346-d6b0a3fa572b"
      },
      "source": [
        "N = 13\n",
        "print(sampled_passages[0:N])\n",
        "print(sampled_queries[0:N])\n",
        "print(sampled_labels[0:N])\n",
        "print(sampled_pids[0:N])\n",
        "print(sampled_qids[0:N])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Disco sensation Andy Gibb dies at the age of 30. With his knee-buckling good looks and his brothers' songwriting talents backing him up, 19-year-old Andy Gibb staged an unprecedented display of youthful pop mastery in the 12 months following his American debut in the spring of 1977.\", 'The contents of this blog, Both poetry and photography have all been registered to Andy Chih Photography and are protected under copyright laws.', 'How old was Aunt Bea when she was on the Andy Griffith Show? The actress and the character were of roughly the same age. Frances Bavier was 57 when The Andy Griffith Show started and 68 when Mayberry R.F.D. (the spinoff of the original …show) was canceled in 1971.', 'According to Oppenheimer, Martha was dead set on marrying upward in social class, and she did. Her husband Andy’s father was a part of the New York Stock Exchange and Andy was a Yale University law student, so they had plenty of money. Before they were even married Andy noticed problems with Martha. One of which was she wasn’t pleased with the engagement ring that he bought her and demanded he return it and buy her one that was bigger, which he did. Then when Andy graduated from law school in 1964, they took a trip to Europe.', 'O. c. cuniculus is thought to be the descendant of early domestic rabbits released into the wild (Gibb 1990), and is now the subspecies that has been introduced throughout Europe and worldwide (Angulo 2004).. c. cuniculus is thought to be the descendant of early domestic rabbits released into the wild (Gibb 1990), and is now the subspecies that has been introduced throughout Europe and worldwide (Angulo 2004).', \"Nor, it seems, in his 28-year marriage to bisexual Dwina, 59, a member of the Daughters of Brahma (a Hindu sect that teaches celibacy) and patroness of the order of the druids. Three years ago, Gibb fathered a daughter, Snow Robin, during an affair with the family's housekeeper, Claire Yang.\", 'Euless, Texas. Euless (/ˈjuːlɪs/ YOO-liss) is a city in Tarrant County, Texas, United States, and a suburb of Fort Worth. Euless is part of the Mid-Cities between Dallas and Fort Worth. The population was 51,277 at the 2010 census.', \"Giddings, Texas. Giddings is the county seat of Lee County, Texas, United States situated on the intersection of U.S. Highways 77 and 290, 55 miles (89 km) east of Austin. Its population was 5,665 at the 2010 census. The city's motto is Giddings Texas: Experience Hometown Hospitality.\", 'Graham, Texas. Graham is a city in north central Texas. It is the county seat of Young County, and as of the 2010 Census had a population of 8,903.', 'Farwell, Texas. Farwell is a city in and the county seat of Parmer County, Texas, United States. The population was 1363 at the 2010 census. The city is located on the Texas-New Mexico border with the town of Texico, New Mexico across the border.', 'Billy Hicks and Doris Meek, comps., Historical Records of Franklin County, Texas (Mount Vernon, Texas: Franklin County Historical Survey Committee, 1972).', 'Center for Health Statistics Texas County Numbers and Public Health Regions Each of the 254 Texas counties has a county number (also know as a county ID) assigned sequentially in alphabetical order. This is not the order in which most computer programs and spreadsheets would sort counties by name.', 'Take some measurements of your driveway and create your own concrete driveway cost calculator. Be sure to include the garage floor if you’re having that replaced. For example, a driveway 14 feet wide and 50 feet long (14×50) contains 700 square feet. Multiplied by the cost of concrete, you’re total concrete driveway cost will be $3,500-$5,600. If you are interested in knowing how much concrete you will use, here’s how to do that.']\n",
            "['did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'did andy gibb die', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'euless texas is in what county?', 'how much does it cost for a concrete driveway']\n",
            "[ 1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1]\n",
            "['2256696', '1250782', '2458296', '1201611', '155226', '2744554', '1091839', '1243511', '1099973', '2671511', '2904686', '1367681', '2603240']\n",
            "['143293', '143293', '143293', '143293', '143293', '143293', '182081', '182081', '182081', '182081', '182081', '182081', '314307']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYDdJnfsnlaQ"
      },
      "source": [
        "**Preprocess Data and data Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmEm2R_oL-w",
        "outputId": "b303ad83-8401-4764-e051-2d7a69a2d611"
      },
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import  WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CC1oGrD75zr"
      },
      "source": [
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4RW-o2n_9n",
        "outputId": "760eae4a-57e7-4bd5-ca68-2b32164198fb"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-31 10:28:17--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.40.142\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.40.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  86.3MB/s    in 17s     \n",
            "\n",
            "2021-03-31 10:28:34 (91.6 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiZeCynoF9n"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8VdP2q4ogG"
      },
      "source": [
        "def process_data(data,rm_stopwords = True, lemm = True):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\t## remove punctuation\n",
        "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
        "\tprocessed_sentences = []\n",
        "\tfor sentence in data:\n",
        "\t\t## tokenise each sentence\n",
        "\t\ttokenised_sentence = tokenizer.tokenize(sentence)\n",
        "\t\t##convert to lower case\n",
        "\t\tsentence = [w.lower() for w in tokenised_sentence]\n",
        "\t\t##exclude non alphabetic words\n",
        "\t\tonly_alpha_sentence = [word for word in sentence if word.isalpha()]\n",
        "\t\t## remove stop words\n",
        "\t\tif(rm_stopwords == True):\n",
        "\t\t\tfiltered_sentence = [w for w in only_alpha_sentence if not w in stop_words]\n",
        "\t\telse:\n",
        "\t\t\tfiltered_sentence = only_alpha_sentence\n",
        "\t\tlemmatized_sentence = []\n",
        "\t\tif(lemm == True):\n",
        "\t\t\t## stemming\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\t#stemmer = PorterStemmer()\n",
        "\t\t\tfor word in filtered_sentence:\n",
        "\t\t\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "\t\t\t\t#lemmatized_sentence.append(stemmer.stem(word))\n",
        "\t\telse:\n",
        "\t\t\tlemmatized_sentence = filtered_sentence\n",
        "\n",
        "\t\tprocessed_sentences.append(lemmatized_sentence)\n",
        "\treturn processed_sentences"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvZjgioo0Nc"
      },
      "source": [
        "train_passages = process_data(sampled_passages)\n",
        "train_queries = process_data(sampled_queries)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cRqgIdl6nDW",
        "outputId": "252980a5-b67c-4005-fa2f-a75ad8b963a6"
      },
      "source": [
        "print(len(train_passages))\n",
        "print(len(train_queries))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27713\n",
            "27713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlfzlqeoSpbW",
        "outputId": "a9164ef2-661e-444d-b714-f114700aa7d9"
      },
      "source": [
        "print(train_passages[0])\n",
        "print(train_queries[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['disco', 'sensation', 'andy', 'gibb', 'dy', 'age', 'knee', 'buckling', 'good', 'look', 'brother', 'songwriting', 'talent', 'backing', 'year', 'old', 'andy', 'gibb', 'staged', 'unprecedented', 'display', 'youthful', 'pop', 'mastery', 'month', 'following', 'american', 'debut', 'spring']\n",
            "['andy', 'gibb', 'die']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcfd6e3pXAXO",
        "outputId": "0b5e70b7-a41b-4160-fe8e-efc20477706d"
      },
      "source": [
        "max_l = 0\n",
        "for sentence in train_passages:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "print(max_l)\n",
        "for sentence in train_queries:\n",
        "  if(len(sentence) > max_l):\n",
        "    max_l = len(sentence)\n",
        "print(max_l)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112\n",
            "112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPx8SdpoXk-o"
      },
      "source": [
        "def find_unique_words(datasets):\n",
        "\n",
        "  # words to indexes\n",
        "  vocab_words = {}\n",
        "  # indexes to embeddings\n",
        "  vocab_embeddings = {}\n",
        "\n",
        "  unique_words = 0\n",
        "\n",
        "  # for every data set training validation testing\n",
        "  for dataset in datasets:\n",
        "    # for every sentence\n",
        "    for sentence in dataset:\n",
        "      # for each word of the sentence\n",
        "      for word in sentence:\n",
        "        # if this word is not saved on the dictionary\n",
        "        if(vocab_words.get(word)==None):\n",
        "          # if this word exists is the word Word2Vec model\n",
        "          if word in word2vec:\n",
        "            unique_words += 1\n",
        "            vocab_words[word] = unique_words\n",
        "            vocab_embeddings[unique_words] = word2vec.wv[word]\n",
        "\n",
        "  return vocab_words,vocab_embeddings"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwTVq4HaXTBX",
        "outputId": "e93006ea-7099-47f4-bd5e-c9601155219e"
      },
      "source": [
        "## vocab dict --> key each unique word and value a unique id\n",
        "## embeddings dict --> key the unique id of a word and value the respective embedding\n",
        "vocab_dict,vocab_embeddings_dict = find_unique_words([train_passages,train_queries])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5el6sTEm1s8o"
      },
      "source": [
        "def get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,sentences):\n",
        "  representations = []\n",
        "  ## for every sentence\n",
        "  for sentence in sentences:\n",
        "    sentence_representation = []\n",
        "    for word in sentence:\n",
        "      ## collect all the word embeddings of the respective words of a sentence\n",
        "      word_index = vocab_dict.get(word)\n",
        "      if(word_index!=None):\n",
        "        word_embedding = vocab_embeddings_dict.get(word_index)\n",
        "        sentence_representation.append(np.array(word_embedding))\n",
        "      else:\n",
        "        sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    if(len(sentence_representation) == 0):\n",
        "      sentence_representation.append(np.array(np.zeros(EMBEDDING_SIZE)))\n",
        "    sentence_representation = np.array(sentence_representation)\n",
        "\n",
        "    representations.append(np.mean(sentence_representation,axis = 0))\n",
        "\n",
        "  return np.array(representations)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv20wAJO34tJ"
      },
      "source": [
        "train_passage_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_passages)\n",
        "train_queries_represenations = get_average_sentence_representation(vocab_dict,vocab_embeddings_dict,train_queries)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTeibggV6cdi",
        "outputId": "f7564051-15fb-4ad3-b306-5828b14b55d9"
      },
      "source": [
        "print(train_passage_represenations.shape)\n",
        "print(train_queries_represenations.shape)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27713, 300)\n",
            "(27713, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LKo-hg-bIv"
      },
      "source": [
        "def combine_representations(q_rep,p_rep,flag = 'diff'):\n",
        "  representation = []\n",
        "  for i,p in enumerate(p_rep):\n",
        "    if(flag == 'diff'):\n",
        "      representation.append((p - q_rep[i]))\n",
        "    else:\n",
        "      representation.append(np.concatenate((p,q_rep[i]),axis = 0))\n",
        "  return np.array(representation)\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb4lB_qq_jvX"
      },
      "source": [
        "train_inputs = combine_representations(train_queries_represenations,train_passage_represenations,flag = 'diff')"
      ],
      "execution_count": 107,
      "outputs": []
    }
  ]
}